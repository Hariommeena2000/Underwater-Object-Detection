{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "702285c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms\n",
    "import torchvision.ops as ops\n",
    "from torch.utils.data import distributed, RandomSampler, SequentialSampler\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1361a416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dee68715",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"DUO/splitted/train\"\n",
    "VAL_PATH = \"DUO/splitted/val\"\n",
    "TEST_PATH = \"DUO/test\"\n",
    "\n",
    "TRAIN_LABELS = TRAIN_PATH + \"/labels\"\n",
    "VAL_LABELS = VAL_PATH + \"/labels\"\n",
    "TEST_LABELS = TEST_PATH + \"/labels\"\n",
    "\n",
    "TRAIN_IMAGES = TRAIN_PATH + \"/images\"\n",
    "VAL_IMAGES = VAL_PATH + \"/images\"\n",
    "TEST_IMAGES = TEST_PATH + \"/images\"\n",
    "\n",
    "# data = \"../data/aquarium_pretrain/data.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d0f2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total Images: 3488\n",
      "‚úÖ Total Labels: 3488\n",
      "üéâ All labels and images match!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_labels_images(image_dir, label_dir, image_exts=(\".jpg\", \".png\", \".jpeg\"), label_ext=\".txt\"):\n",
    "    image_files = set(os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(image_exts))\n",
    "    label_files = set(os.path.splitext(f)[0] for f in os.listdir(label_dir) if f.endswith(label_ext))\n",
    "\n",
    "    missing_images = label_files - image_files\n",
    "    missing_labels = image_files - label_files\n",
    "\n",
    "    print(f\"‚úÖ Total Images: {len(image_files)}\")\n",
    "    print(f\"‚úÖ Total Labels: {len(label_files)}\")\n",
    "\n",
    "    if missing_images:\n",
    "        print(f\"‚ùå Labels without images ({len(missing_images)}):\")\n",
    "        for name in sorted(missing_images):\n",
    "            print(f\" - {name + '.txt'}\")\n",
    "\n",
    "    if missing_labels:\n",
    "        print(f\"‚ùå Images without labels ({len(missing_labels)}):\")\n",
    "        for name in sorted(missing_labels):\n",
    "            print(f\" - {name + '.jpg'}\")\n",
    "\n",
    "    if not missing_images and not missing_labels:\n",
    "        print(\"üéâ All labels and images match!\")\n",
    "\n",
    "# Example usage:\n",
    "check_labels_images(\n",
    "    image_dir=TRAIN_IMAGES,\n",
    "    label_dir=TRAIN_LABELS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4cf2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION_PATH = \"DUO\"\n",
    "\n",
    "TRAIN_DESTINATION = DESTINATION_PATH + \"/train2\"\n",
    "VAL_DESTINATION = DESTINATION_PATH + \"/val2\"\n",
    "TEST_DESTINATION = DESTINATION_PATH + \"/test2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/4...\n",
      "Processing image 2/4...\n",
      "Processing image 3/4...\n",
      "Processing image 4/4...\n",
      "Saved mosaic image and labels to DUO/train2\\2370_2371_2372_2373.jpg and DUO/train2/labels\\2370_2371_2372_2373.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0ef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "‚úÖ **All YOLO label coordinates are normalized in [0.0, 1.0].**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Notebook cell: Verify YOLO label normalization\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# # === USER EDITABLE: point this to your labels folder ===\n",
    "# label_dir = TRAIN_LABELS\n",
    "\n",
    "# # collect all .txt files\n",
    "# label_files = glob.glob(os.path.join(label_dir, \"*.txt\"))\n",
    "\n",
    "# out_of_range = []\n",
    "# for lf in label_files:\n",
    "#     with open(lf, 'r') as f:\n",
    "#         for line_no, line in enumerate(f, start=1):\n",
    "#             parts = line.strip().split()\n",
    "#             if len(parts) != 5:\n",
    "#                 continue\n",
    "#             cls, xc, yc, w, h = parts\n",
    "#             coords = {\n",
    "#                 \"x_center\": float(xc),\n",
    "#                 \"y_center\": float(yc),\n",
    "#                 \"width\":     float(w),\n",
    "#                 \"height\":    float(h)\n",
    "#             }\n",
    "#             for name, val in coords.items():\n",
    "#                 if not (0.0 <= val <= 1.0):\n",
    "#                     out_of_range.append({\n",
    "#                         \"file\": os.path.basename(lf),\n",
    "#                         \"line\": line_no,\n",
    "#                         \"coord\": name,\n",
    "#                         \"value\": val\n",
    "#                     })\n",
    "\n",
    "# df = pd.DataFrame(out_of_range)\n",
    "\n",
    "# if df.empty:\n",
    "#     display(Markdown(\"‚úÖ **All YOLO label coordinates are normalized in [0.0, 1.0].**\"))\n",
    "# else:\n",
    "#     display(Markdown(f\"‚ö†Ô∏è **Found {len(df)} out‚Äëof‚Äërange coordinate(s):**\"))\n",
    "#     display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f53e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# import os\n",
    "# import json\n",
    "# import argparse\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def coco_to_yolo(coco_json_path, output_dir):\n",
    "#     # Load COCO JSON\n",
    "#     with open(coco_json_path, 'r') as f:\n",
    "#         coco = json.load(f)\n",
    "\n",
    "#     # Build image metadata lookup: image_id -> (file_name, width, height)\n",
    "#     images = {img['id']: (img['file_name'], img['width'], img['height'])\n",
    "#               for img in coco.get('images', [])}\n",
    "\n",
    "#     # Build annotations grouped by image_id\n",
    "#     annos_per_image = defaultdict(list)\n",
    "#     for ann in coco.get('annotations', []):\n",
    "#         image_id = ann['image_id']\n",
    "#         annos_per_image[image_id].append(ann)\n",
    "\n",
    "#     # Build category_id ‚Üí new sequential class_id mapping (0‚Ä¶N-1)\n",
    "#     all_cat_ids = sorted({cat['id'] for cat in coco.get('categories', [])})\n",
    "#     catid2class = {cat_id: idx for idx, cat_id in enumerate(all_cat_ids)}\n",
    "#     print(\"Category ID to class index mapping:\")\n",
    "#     for cat_id, cls in catid2class.items():\n",
    "#         print(f\"  COCO cat {cat_id} ‚Üí YOLO cls {cls}\")\n",
    "\n",
    "#     # Make sure output directory exists\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Process each image\n",
    "#     for image_id, (file_name, width, height) in images.items():\n",
    "#         yolo_lines = []\n",
    "#         for ann in annos_per_image.get(image_id, []):\n",
    "#             bbox = ann['bbox']  # [x_min, y_min, box_w, box_h]\n",
    "#             x, y, w, h = bbox\n",
    "#             # Compute centers\n",
    "#             x_c = x + w / 2\n",
    "#             y_c = y + h / 2\n",
    "\n",
    "#             # Normalize\n",
    "#             x_c_norm = x_c / width\n",
    "#             y_c_norm = y_c / height\n",
    "#             w_norm   = w   / width\n",
    "#             h_norm   = h   / height\n",
    "\n",
    "#             # Check & warn if any not in [0,1]\n",
    "#             for name, val in [('x_center', x_c_norm), ('y_center', y_c_norm),\n",
    "#                               ('w', w_norm), ('h', h_norm)]:\n",
    "#                 if not (0.0 <= val <= 1.0):\n",
    "#                     print(f\"WARNING: {name}={val:.4f} out of [0,1] for image '{file_name}'. \"\n",
    "#                           \"Normalizing anyway.\")\n",
    "#                     # Clip to [0,1]\n",
    "#                     val_clipped = max(0.0, min(1.0, val))\n",
    "#                     if name == 'x_center':\n",
    "#                         x_c_norm = val_clipped\n",
    "#                     elif name == 'y_center':\n",
    "#                         y_c_norm = val_clipped\n",
    "#                     elif name == 'w':\n",
    "#                         w_norm = val_clipped\n",
    "#                     elif name == 'h':\n",
    "#                         h_norm = val_clipped\n",
    "\n",
    "#             cls_id = catid2class.get(ann['category_id'], None)\n",
    "#             if cls_id is None:\n",
    "#                 print(f\"WARNING: annotation with unknown category_id {ann['category_id']} skipped.\")\n",
    "#                 continue\n",
    "\n",
    "#             yolo_lines.append(f\"{cls_id} \"\n",
    "#                               f\"{x_c_norm:.6f} {y_c_norm:.6f} \"\n",
    "#                               f\"{w_norm:.6f} {h_norm:.6f}\")\n",
    "\n",
    "#         # Write .txt file (same base name as image)\n",
    "#         base, _ = os.path.splitext(file_name)\n",
    "#         out_path = os.path.join(output_dir, base + '.txt')\n",
    "#         with open(out_path, 'w') as f:\n",
    "#             f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "#     print(f\"Done! YOLO annotations saved to '{output_dir}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de767c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco_to_yolo(\n",
    "#     coco_json_path='D:/underwater_OD/DUO/test/instances_test.json',\n",
    "#     output_dir='D:/underwater_OD/DUO/test/labels'  # or './DUO/train'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67d2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# def split_dataset(images_dir, labels_dir, output_dir,\n",
    "#                   train_ratio=0.8, seed=None, move_val=False):\n",
    "#     \"\"\"\n",
    "#     Split a YOLO-formatted dataset into train and validation sets.\n",
    "\n",
    "#     Args:\n",
    "#         images_dir (str): Directory containing image files.\n",
    "#         labels_dir (str): Directory containing YOLO .txt label files.\n",
    "#         output_dir (str): Root directory where 'train' and 'val' folders will be created.\n",
    "#         train_ratio (float): Fraction of data to use for training (default: 0.8).\n",
    "#         seed (int, optional): Random seed for reproducibility.\n",
    "#         move_val (bool): If True, move val files out of source dirs (removing them).\n",
    "#     \"\"\"\n",
    "#     # Make output subdirectories\n",
    "#     for split in ('train', 'val'):\n",
    "#         for sub in ('images', 'labels'):\n",
    "#             os.makedirs(os.path.join(output_dir, split, sub), exist_ok=True)\n",
    "\n",
    "#     # Gather image files\n",
    "#     exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n",
    "#     image_files = [f for f in os.listdir(images_dir)\n",
    "#                    if os.path.splitext(f)[1].lower() in exts]\n",
    "\n",
    "#     # Pair images with labels\n",
    "#     pairs = []\n",
    "#     for img in image_files:\n",
    "#         base = os.path.splitext(img)[0]\n",
    "#         label_path = os.path.join(labels_dir, base + '.txt')\n",
    "#         if os.path.exists(label_path):\n",
    "#             pairs.append((img, label_path))\n",
    "#         else:\n",
    "#             print(f\"WARNING: Missing label for {img}, skipping.\")\n",
    "\n",
    "#     # Shuffle and split\n",
    "#     if seed is not None:\n",
    "#         random.seed(seed)\n",
    "#     random.shuffle(pairs)\n",
    "#     n_train = int(len(pairs) * train_ratio)\n",
    "#     train_pairs = pairs[:n_train]\n",
    "#     val_pairs = pairs[n_train:]\n",
    "\n",
    "#     # Copy train pairs\n",
    "#     for img_file, lbl_path in train_pairs:\n",
    "#         src_img = os.path.join(images_dir, img_file)\n",
    "#         dst_img = os.path.join(output_dir, 'train', 'images', img_file)\n",
    "#         shutil.copy2(src_img, dst_img)\n",
    "#         dst_lbl = os.path.join(output_dir, 'train', 'labels', os.path.basename(lbl_path))\n",
    "#         shutil.copy2(lbl_path, dst_lbl)\n",
    "\n",
    "#     # Handle val pairs: copy or move\n",
    "#     for img_file, lbl_path in val_pairs:\n",
    "#         src_img = os.path.join(images_dir, img_file)\n",
    "#         dst_img = os.path.join(output_dir, 'val', 'images', img_file)\n",
    "#         src_lbl = lbl_path\n",
    "#         dst_lbl = os.path.join(output_dir, 'val', 'labels', os.path.basename(lbl_path))\n",
    "\n",
    "#         if move_val:\n",
    "#             shutil.move(src_img, dst_img)\n",
    "#             shutil.move(src_lbl, dst_lbl)\n",
    "#         else:\n",
    "#             shutil.copy2(src_img, dst_img)\n",
    "#             shutil.copy2(src_lbl, dst_lbl)\n",
    "\n",
    "#     action = 'moved' if move_val else 'copied'\n",
    "#     print(f\"Split complete: {len(train_pairs)} train (copied), {len(val_pairs)} val ({action}).\")\n",
    "\n",
    "# # Example usage in a notebook cell:\n",
    "# # split_dataset(\n",
    "# #     images_dir='D:/underwater_OD/DUO/train/images',\n",
    "# #     labels_dir='D:/underwater_OD/DUO/train/labels',\n",
    "# #     output_dir='D:/underwater_OD/DUO',\n",
    "# #     train_ratio=0.8,\n",
    "# #     seed=42,\n",
    "# #     move_val=True  # remove val images from train dir\n",
    "# # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5327622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_dataset(\n",
    "#     images_dir='D:/underwater_OD/DUO/train_original/images',\n",
    "#     labels_dir='D:/underwater_OD/DUO/train_original/labels',\n",
    "#     output_dir='D:/underwater_OD/DUO/splitted',\n",
    "#     train_ratio=0.8,\n",
    "#     seed=42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18aab60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to Label Mapping: {0: 'holothurian', 1: 'echinus', 2: 'scallop', 3: 'starfish'}\n",
      "Label to Index Mapping: {'holothurian': 0, 'echinus': 1, 'scallop': 2, 'starfish': 3}\n"
     ]
    }
   ],
   "source": [
    "classes = [\"holothurian\", \"echinus\", \"scallop\",\"starfish\"]\n",
    "Idx2Label = {idx: label for idx, label in enumerate(classes)}\n",
    "Label2Index = {label: idx for idx, label in Idx2Label.items()}\n",
    "print(\"Index to Label Mapping:\", Idx2Label)\n",
    "print(\"Label to Index Mapping:\", Label2Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a214d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_num_images(data, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.bar(data.keys(), data.values())\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def get_num_images(path):\n",
    "    return len(os.listdir(path))\n",
    "\n",
    "plot_num_images({ \"train\": get_num_images(TRAIN_IMAGES), \"val\": get_num_images(VAL_IMAGES), \"test\": get_num_images(TEST_IMAGES) }, \"Number of Images in Each Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a45c530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3488/3488 [00:00<00:00, 22418.81it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1335/1335 [00:00<00:00, 7257.39it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3488/3488 [00:00<00:00, 21136.38it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_objects_per_images(image_dir, label_dir, object_count, total_object_count):\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    for image_file in tqdm(image_files):\n",
    "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
    "        f = open(label_path, \"r\")\n",
    "        lines = f.readlines()\n",
    "        total_object_count.append(len(lines))\n",
    "        for line in lines:\n",
    "            object_count[Idx2Label[int(line.split()[0])]] = object_count.get(Idx2Label[int(line.split()[0])], 0) + 1\n",
    "        f.close()\n",
    "        \n",
    "train_object_count = {}\n",
    "val_object_count = {}\n",
    "test_object_count = {}\n",
    "\n",
    "total_object_per_train_count = []\n",
    "total_object_per_val_count = []\n",
    "total_object_per_test_count = []\n",
    "\n",
    "count_objects_per_images(TRAIN_IMAGES, TRAIN_LABELS, train_object_count, total_object_per_train_count)\n",
    "count_objects_per_images(VAL_IMAGES, VAL_LABELS, val_object_count, total_object_per_val_count)\n",
    "count_objects_per_images(TEST_IMAGES, TEST_LABELS, test_object_count, total_object_per_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3501489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'echinus': 22423, 'holothurian': 3614, 'scallop': 901, 'starfish': 6388}\n",
      "\n",
      "{'echinus': 8870, 'holothurian': 1324, 'scallop': 375, 'starfish': 2547}\n",
      "\n",
      "{'echinus': 22423, 'holothurian': 3614, 'scallop': 901, 'starfish': 6388}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(train_object_count)\n",
    "print()\n",
    "pp.pprint(val_object_count)\n",
    "print()\n",
    "pp.pprint(test_object_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_object_count(object_count, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(object_count.keys(), object_count.values())\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_object_count(train_object_count, \"Train Object Count\")\n",
    "plot_object_count(val_object_count, \"Validation Object Count\")\n",
    "plot_object_count(test_object_count, \"Test Object Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ce4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_object_count = {label: train_object_count.get(label, 0) + val_object_count.get(label, 0) + test_object_count.get(label, 0) for label in classes}\n",
    "plot_object_count(total_object_count, \"Total Object Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_object_per_image(total_object_per_image, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(total_object_per_image, bins=20)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_total_object_per_image(total_object_per_train_count, \"Total Object Per Train Image\")\n",
    "plot_total_object_per_image(total_object_per_val_count, \"Total Object Per Validation Image\")\n",
    "plot_total_object_per_image(total_object_per_test_count, \"Total Object Per Test Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ee1aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3488/3488 [00:00<00:00, 21364.95it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1335/1335 [00:00<00:00, 23067.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3488/3488 [00:00<00:00, 21227.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_bbox_sizes(image_dir, label_dir, sizes):\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    for image_file in tqdm(image_files):\n",
    "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
    "        f = open(label_path, \"r\")\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sizes.append(float(line.split()[3]) * float(line.split()[4]))\n",
    "        f.close()\n",
    "        \n",
    "train_sizes = []\n",
    "val_sizes = []\n",
    "test_sizes = []\n",
    "\n",
    "get_bbox_sizes(TRAIN_IMAGES, TRAIN_LABELS, train_sizes)\n",
    "get_bbox_sizes(VAL_IMAGES, VAL_LABELS, val_sizes)\n",
    "get_bbox_sizes(TEST_IMAGES, TEST_LABELS, test_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c79dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox_sizes(sizes, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(sizes, bins=20)\n",
    "    plt.title(title)\n",
    "    plt.xlim(0, .2)\n",
    "    plt.show()\n",
    "    \n",
    "plot_bbox_sizes(train_sizes, \"Train Bounding Box Sizes\")\n",
    "plot_bbox_sizes(val_sizes, \"Validation Bounding Box Sizes\")\n",
    "plot_bbox_sizes(test_sizes, \"Test Bounding Box Sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b41803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_bbox_sizes(train_sizes, val_sizes, test_sizes, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(train_sizes, bins=20, alpha=0.5, label=\"Train\")\n",
    "    plt.hist(val_sizes, bins=20, alpha=0.5, label=\"Validation\")\n",
    "    plt.hist(test_sizes, bins=20, alpha=0.5, label=\"Test\")\n",
    "    plt.title(title)\n",
    "    plt.xlim(0, .2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_all_bbox_sizes(train_sizes, val_sizes, test_sizes, \"All Bounding Box Sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e52c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_width_height(image_dir, label_dir, width_height):\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    for image_file in tqdm(image_files):\n",
    "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
    "        f = open(label_path, \"r\")\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            width_height.append((float(line.split()[3]), float(line.split()[4])))\n",
    "        f.close()\n",
    "        \n",
    "train_width_height = []\n",
    "val_width_height = []\n",
    "test_width_height = []\n",
    "\n",
    "count_width_height(TRAIN_IMAGES, TRAIN_LABELS, train_width_height)\n",
    "count_width_height(VAL_IMAGES, VAL_LABELS, val_width_height)\n",
    "count_width_height(TEST_IMAGES, TEST_LABELS, test_width_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dad5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_width_height(width_height, title):\n",
    "    width, height = zip(*width_height)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(width, bins=20, alpha=0.5, label=\"Width\")\n",
    "    plt.hist(height, bins=20, alpha=0.5, label=\"Height\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_width_height(train_width_height, \"Train Width and Height\")\n",
    "plot_width_height(val_width_height, \"Validation Width and Height\")\n",
    "plot_width_height(test_width_height, \"Test Width and Height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83882043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_width_height_scatter(width_height, title):\n",
    "    width, height = zip(*width_height)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(width, height)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_width_height_scatter(train_width_height, \"Train Width vs Height\")\n",
    "plot_width_height_scatter(val_width_height, \"Validation Width vs Height\")\n",
    "plot_width_height_scatter(test_width_height, \"Test Width vs Height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_width_height_scatter(train_width_height, val_width_height, test_width_height, title):\n",
    "    train_width, train_height = zip(*train_width_height)\n",
    "    val_width, val_height = zip(*val_width_height)\n",
    "    test_width, test_height = zip(*test_width_height)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(train_width, train_height, alpha=0.5, label=\"Train\")\n",
    "    plt.scatter(val_width, val_height, alpha=0.5, label=\"Validation\")\n",
    "    plt.scatter(test_width, test_height, alpha=0.5, label=\"Test\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_all_width_height_scatter(train_width_height, val_width_height, test_width_height, \"All Width vs Height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_aspect_ratios(train_width_height, val_width_height, test_width_height, title):\n",
    "    EPSILON = 1e-6\n",
    "            \n",
    "    train_aspect_ratio = [w / h if h != 0 else w / (h + EPSILON) for w, h in train_width_height] \n",
    "    val_aspect_ratio = [w / h if h != 0 else w / (h + EPSILON) for w, h in val_width_height]\n",
    "    test_aspect_ratio = [w / h if h != 0 else w / (h + EPSILON) for w, h in test_width_height]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(train_aspect_ratio, alpha=0.5, label=\"Train\", bins=50)\n",
    "    plt.hist(val_aspect_ratio, alpha=0.5, label=\"Validation\", bins=50)\n",
    "    plt.hist(test_aspect_ratio, alpha=0.5, label=\"Test\", bins=50)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_all_aspect_ratios(train_width_height, val_width_height, test_width_height, \"All Aspect Ratios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48868e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_with_annotation_bounding_boxes(image_dir, label_dir):\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "\n",
    "    sample_image_files = random.sample(image_files, 12)\n",
    "\n",
    "    fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
    "\n",
    "    for i, image_file in enumerate(sample_image_files):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
    "        f = open(label_path, \"r\")\n",
    "\n",
    "        for label in f:\n",
    "            class_id, x_center, y_center, width, height = map(float, label.split())\n",
    "            h, w, _ = image.shape\n",
    "            x_min = int((x_center - width / 2) * w)\n",
    "            y_min = int((y_center - height / 2) * h)\n",
    "            x_max = int((x_center + width / 2) * w)\n",
    "            y_max = int((y_center + height / 2) * h)\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                Idx2Label[int(class_id)],\n",
    "                (x_min, y_min),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=1,\n",
    "                color=(255, 255, 255),\n",
    "                thickness=2,\n",
    "            )\n",
    "\n",
    "        axs[row, col].imshow(image)\n",
    "        axs[row, col].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_image_with_annotation_bounding_boxes(TRAIN_IMAGES, TRAIN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "351ede14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_labels(\n",
    "#     image_dir='DUO/val/images',\n",
    "#     label_dir='DUO/val/labels',\n",
    "#     image_dest='DUO/val_norm/images',\n",
    "#     label_dest='DUO/val_norm/labels',\n",
    "#     img_size=640,\n",
    "#     square=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab40157",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_IMAGES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 386\u001b[39m\n\u001b[32m    371\u001b[39m     cv2.putText(\n\u001b[32m    372\u001b[39m         img,\n\u001b[32m    373\u001b[39m         text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    379\u001b[39m         cv2.LINE_AA,\n\u001b[32m    380\u001b[39m     )\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[32m    384\u001b[39m mosaic(\n\u001b[32m    385\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33m2370.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2371.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2372.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2373.jpg\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[43mTRAIN_IMAGES\u001b[49m,\n\u001b[32m    387\u001b[39m     TRAIN_LABELS,\n\u001b[32m    388\u001b[39m     TRAIN_DESTINATION,\n\u001b[32m    389\u001b[39m     TRAIN_DESTINATION+\u001b[33m'\u001b[39m\u001b[33m/labels\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    390\u001b[39m     \u001b[32m640\u001b[39m,\n\u001b[32m    391\u001b[39m     \u001b[32m640\u001b[39m,\n\u001b[32m    392\u001b[39m     \u001b[32m0.5\u001b[39m,\n\u001b[32m    393\u001b[39m     \u001b[32m0.5\u001b[39m,\n\u001b[32m    394\u001b[39m     \u001b[32m0.1\u001b[39m,\n\u001b[32m    395\u001b[39m     \u001b[32m0.3\u001b[39m,\n\u001b[32m    396\u001b[39m     show_image=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    397\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'TRAIN_IMAGES' is not defined"
     ]
    }
   ],
   "source": [
    "def get_train_aug():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.Blur(blur_limit=3, p=0.5),\n",
    "                    A.MotionBlur(blur_limit=3, p=0.5),\n",
    "                    A.MedianBlur(blur_limit=3, p=0.5),\n",
    "                ],\n",
    "                p=0.5,\n",
    "            ),\n",
    "            A.ToGray(p=0.1),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            A.RandomGamma(p=0.1),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\",\n",
    "            label_fields=[\"labels\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\",\n",
    "            label_fields=[\"labels\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\",\n",
    "            label_fields=[\"labels\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def infer_transforms(image):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    return transform(image)\n",
    "\n",
    "\n",
    "def preprocess(image_name, image_dir, label_dir):\n",
    "    image_path = os.path.sep.join([image_dir, image_name])\n",
    "    name = image_name.split(\".\")[0]\n",
    "    label_name = name + \".txt\"\n",
    "    label_path = os.path.sep.join([label_dir, label_name])\n",
    "\n",
    "    return image_path, label_path, label_name\n",
    "\n",
    "\n",
    "def draw_rect(img, bboxes, color=(255, 0, 0)):\n",
    "    img = img.copy()\n",
    "    height, width = img.shape[:2]\n",
    "    for bbox in bboxes:\n",
    "        center_x, center_y, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        x = int((center_x - w / 2) * width)\n",
    "        w = int(w * width)\n",
    "        y = int((center_y - h / 2) * height)\n",
    "        h = int(h * height)\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), color, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_img(image_path, cvt_color=True):\n",
    "    img = cv2.imread(image_path)\n",
    "    if cvt_color:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_img(image, save_path, jpg_quality=None):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if jpg_quality:\n",
    "        cv2.imwrite(save_path, image, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n",
    "    else:\n",
    "        cv2.imwrite(save_path, image)\n",
    "\n",
    "\n",
    "def read_label(label_path):\n",
    "    with open(label_path) as f:\n",
    "        conts = f.readlines()\n",
    "\n",
    "    bboxes = []\n",
    "    class_labels = []\n",
    "    for cont in conts:\n",
    "        cont = cont.strip().split()\n",
    "        center_x, center_y, w, h = (\n",
    "            float(cont[1]),\n",
    "            float(cont[2]),\n",
    "            float(cont[3]),\n",
    "            float(cont[4]),\n",
    "        )\n",
    "        bboxes.append([center_x, center_y, w, h])\n",
    "        class_labels.append(cont[0])\n",
    "    return (bboxes, class_labels)\n",
    "\n",
    "\n",
    "def display_img(image_path, label_path):\n",
    "    img = read_img(image_path, cvt_color=False)\n",
    "    bboxes = read_label(label_path)[0]\n",
    "    img = draw_rect(img, bboxes)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "def save_label(bboxes, class_labels, label_path):\n",
    "    tem_lst = []\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        tem_lst.append(\n",
    "            class_labels[i]\n",
    "            + \" \"\n",
    "            + str(bbox[0])\n",
    "            + \" \"\n",
    "            + str(bbox[1])\n",
    "            + \" \"\n",
    "            + str(bbox[2])\n",
    "            + \" \"\n",
    "            + str(bbox[3])\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "    with open(label_path, \"w\") as f:\n",
    "        f.writelines(tem_lst)\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def random_crop_savebboxes(\n",
    "    image_name, image_dir, label_dir, expected_h, expected_w, min_area, min_visibility\n",
    "):\n",
    "    image_path, label_path, _ = preprocess(image_name, image_dir, label_dir)\n",
    "\n",
    "    (bboxes, class_labels) = read_label(label_path)\n",
    "\n",
    "    transform = A.Compose(\n",
    "        [A.RandomResizedCrop(size=(expected_h, expected_w))],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"yolo\",\n",
    "            label_fields=[\"class_labels\"],\n",
    "            min_area=min_area,\n",
    "            min_visibility=min_visibility,\n",
    "        ),\n",
    "    )\n",
    "    bboxes = np.clip(bboxes, 0.0, 1.0)  # <-- clamp values\n",
    "# transformed = transform(image=read_img(image_path), bboxes=bboxes, class_labels=class_labels)\n",
    "\n",
    "    transformed = transform(image=read_img(image_path), bboxes=bboxes, class_labels=class_labels)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    transformed_bboxes = transformed[\"bboxes\"]\n",
    "    transformed_class_labels = transformed[\"class_labels\"]\n",
    "\n",
    "    return transformed_image, transformed_bboxes, transformed_class_labels\n",
    "\n",
    "\n",
    "def mosaic(\n",
    "    image_file_list,\n",
    "    image_dir,\n",
    "    label_dir,\n",
    "    output_image_dir,\n",
    "    output_label_dir,\n",
    "    mo_w,\n",
    "    mo_h,\n",
    "    scale_x,\n",
    "    scale_y,\n",
    "    min_area,\n",
    "    min_visibility,\n",
    "    show_image=False,\n",
    "):\n",
    "    # Ensure output directories exist\n",
    "    if not os.path.exists(output_image_dir):\n",
    "        os.makedirs(output_image_dir)\n",
    "\n",
    "    if not os.path.exists(output_label_dir):\n",
    "        os.makedirs(output_label_dir)\n",
    "\n",
    "    new_img = np.zeros((mo_h, mo_w, 3), dtype=\"uint8\")\n",
    "\n",
    "    div_point_x = int(mo_w * scale_x)\n",
    "    div_point_y = int(mo_h * scale_y)\n",
    "\n",
    "    # Initialize variables to store all bounding boxes and labels\n",
    "    new_bboxes = []\n",
    "    new_class_labels = []\n",
    "\n",
    "    for i in range(len(image_file_list)):\n",
    "        print(f\"Processing image {i + 1}/{len(image_file_list)}...\")\n",
    "\n",
    "        if i == 0:\n",
    "            w0 = div_point_x\n",
    "            h0 = div_point_y\n",
    "            img_0, bboxes_0, class_labels_0 = random_crop_savebboxes(\n",
    "                image_file_list[0],\n",
    "                image_dir,\n",
    "                label_dir,\n",
    "                h0,\n",
    "                w0,\n",
    "                min_area,\n",
    "                min_visibility,\n",
    "            )\n",
    "            new_img[:div_point_y, :div_point_x, :] = img_0\n",
    "\n",
    "            if len(bboxes_0) == 0:\n",
    "                bboxes_0_new = []\n",
    "            else:\n",
    "                bboxes_0_new = np.zeros((len(bboxes_0), 4))\n",
    "                bboxes_0_new = bboxes_0_new.tolist()\n",
    "\n",
    "            for i, box in enumerate(bboxes_0):\n",
    "                bboxes_0_new[i][0] = box[0] * scale_x\n",
    "                bboxes_0_new[i][2] = box[2] * scale_x\n",
    "\n",
    "                bboxes_0_new[i][1] = box[1] * scale_y\n",
    "                bboxes_0_new[i][3] = box[3] * scale_y\n",
    "\n",
    "            new_bboxes.extend(bboxes_0_new)\n",
    "            new_class_labels.extend(class_labels_0)\n",
    "\n",
    "        elif i == 1:\n",
    "            w1 = mo_w - div_point_x\n",
    "            h1 = div_point_y\n",
    "            img_1, bboxes_1, class_labels_1 = random_crop_savebboxes(\n",
    "                image_file_list[1],\n",
    "                image_dir,\n",
    "                label_dir,\n",
    "                h1,\n",
    "                w1,\n",
    "                min_area,\n",
    "                min_visibility,\n",
    "            )\n",
    "            new_img[:div_point_y, div_point_x:, :] = img_1\n",
    "\n",
    "            if len(bboxes_1) == 0:\n",
    "                bboxes_1_new = []\n",
    "            else:\n",
    "                bboxes_1_new = np.zeros((len(bboxes_1), 4))\n",
    "                bboxes_1_new = bboxes_1_new.tolist()\n",
    "\n",
    "            for i, box in enumerate(bboxes_1):\n",
    "                bboxes_1_new[i][0] = box[0] * (1 - scale_x) + scale_x\n",
    "                bboxes_1_new[i][2] = box[2] * (1 - scale_x)\n",
    "\n",
    "                bboxes_1_new[i][1] = box[1] * scale_y\n",
    "                bboxes_1_new[i][3] = box[3] * scale_y\n",
    "\n",
    "            new_bboxes.extend(bboxes_1_new)\n",
    "            new_class_labels.extend(class_labels_1)\n",
    "\n",
    "        elif i == 2:\n",
    "            w2 = div_point_x\n",
    "            h2 = mo_h - div_point_y\n",
    "            img_2, bboxes_2, class_labels_2 = random_crop_savebboxes(\n",
    "                image_file_list[2],\n",
    "                image_dir,\n",
    "                label_dir,\n",
    "                h2,\n",
    "                w2,\n",
    "                min_area,\n",
    "                min_visibility,\n",
    "            )\n",
    "            new_img[div_point_y:, :div_point_x, :] = img_2\n",
    "\n",
    "            if len(bboxes_2) == 0:\n",
    "                bboxes_2_new = []\n",
    "            else:\n",
    "                bboxes_2_new = np.zeros((len(bboxes_2), 4))\n",
    "                bboxes_2_new = bboxes_2_new.tolist()\n",
    "\n",
    "            for i, box in enumerate(bboxes_2):\n",
    "                bboxes_2_new[i][0] = box[0] * scale_x\n",
    "                bboxes_2_new[i][2] = box[2] * scale_x\n",
    "\n",
    "                bboxes_2_new[i][1] = box[1] * (1 - scale_y) + scale_y\n",
    "                bboxes_2_new[i][3] = box[3] * (1 - scale_y)\n",
    "\n",
    "            new_bboxes.extend(bboxes_2_new)\n",
    "            new_class_labels.extend(class_labels_2)\n",
    "\n",
    "        else:\n",
    "            w3 = mo_w - div_point_x\n",
    "            h3 = mo_h - div_point_y\n",
    "            img_3, bboxes_3, class_labels_3 = random_crop_savebboxes(\n",
    "                image_file_list[3],\n",
    "                image_dir,\n",
    "                label_dir,\n",
    "                h3,\n",
    "                w3,\n",
    "                min_area,\n",
    "                min_visibility,\n",
    "            )\n",
    "            new_img[div_point_y:, div_point_x:, :] = img_3\n",
    "\n",
    "            if len(bboxes_3) == 0:\n",
    "                bboxes_3_new = []\n",
    "            else:\n",
    "                bboxes_3_new = np.zeros((len(bboxes_3), 4))\n",
    "                bboxes_3_new = bboxes_3_new.tolist()\n",
    "\n",
    "            for i, box in enumerate(bboxes_3):\n",
    "                bboxes_3_new[i][0] = box[0] * (1 - scale_x) + scale_x\n",
    "                bboxes_3_new[i][2] = box[2] * (1 - scale_x)\n",
    "\n",
    "                bboxes_3_new[i][1] = box[1] * (1 - scale_y) + scale_y\n",
    "                bboxes_3_new[i][3] = box[3] * (1 - scale_y)\n",
    "\n",
    "            new_bboxes.extend(bboxes_3_new)\n",
    "            new_class_labels.extend(class_labels_3)\n",
    "\n",
    "    # Generate unique output file names\n",
    "    mosaic_image_name = \"_\".join([img.split(\".\")[0] for img in image_file_list]) + \".jpg\"\n",
    "    mosaic_label_name = \"_\".join([img.split(\".\")[0] for img in image_file_list]) + \".txt\"\n",
    "\n",
    "    # Paths for saving the mosaic image and label\n",
    "    image_store_path = os.path.join(output_image_dir, mosaic_image_name)\n",
    "    label_store_path = os.path.join(output_label_dir, mosaic_label_name)\n",
    "\n",
    "    # Save the mosaic image and bounding box labels\n",
    "    save_img(new_img, image_store_path)\n",
    "    save_label(new_bboxes, new_class_labels, label_store_path)\n",
    "    l,.2.imshow(\"Mosaic Image\", new_img)\n",
    "    cv2.waitKey(1)  # Non-blocking\n",
    "\n",
    "    print(f\"Saved mosaic image and labels to {image_store_path} and {label_store_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def draw_text(\n",
    "    img,\n",
    "    text,\n",
    "    font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    pos=(0, 0),\n",
    "    font_scale=1,\n",
    "    font_thickness=2,\n",
    "    text_color=(0, 255, 0),\n",
    "    text_color_bg=(0, 0, 0),\n",
    "):\n",
    "    offset = (5, 5)\n",
    "    x, y = pos\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    rec_start = tuple(x - y for x, y in zip(pos, offset))\n",
    "    rec_end = tuple(x + y for x, y in zip((x + text_w, y + text_h), offset))\n",
    "    cv2.rectangle(img, rec_start, rec_end, text_color_bg, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text,\n",
    "        (x, int(y + text_h + font_scale - 1)),\n",
    "        font,\n",
    "        font_scale,\n",
    "        text_color,\n",
    "        font_thickness,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "mosaic(\n",
    "    [\"2370.jpg\", \"2371.jpg\", \"2372.jpg\", \"2373.jpg\"],\n",
    "    TRAIN_IMAGES,\n",
    "    TRAIN_LABELS,\n",
    "    TRAIN_DESTINATION,\n",
    "    TRAIN_DESTINATION+'/labels',\n",
    "    640,\n",
    "    640,\n",
    "    0.5,\n",
    "    0.5,\n",
    "    0.1,\n",
    "    0.3,\n",
    "    show_image=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d7c19",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "We will create a custom dataset class to load the images and annotations from the dataset and apply the data augmentation techniques. We will also create a data loader to load the data in batches and shuffle it during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec57d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_path,\n",
    "        labels_path,\n",
    "        img_size=(640, 640),\n",
    "        classes=None,\n",
    "        transforms=None,\n",
    "        use_train_aug=False,\n",
    "        train=False,\n",
    "        mosaic=1.0,\n",
    "        square_training=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_path (str): Path to the images directory.\n",
    "            labels_path (str): Path to the labels directory.\n",
    "            img_size (tuple): Desired image size (width, height).\n",
    "            classes (list, optional): List of class labels (unused but kept for interface).\n",
    "            transforms (callable, optional): Transformations to be applied to the images.\n",
    "            use_train_aug (bool, optional): Unused augmentation flag (kept for compatibility).\n",
    "            train (bool, optional): Unused training flag (kept for compatibility).\n",
    "            mosaic (float, optional): Unused mosaic factor (kept for compatibility).\n",
    "            square_training (bool, optional): Unused square training flag.\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "        # Store additional flags for compatibility\n",
    "        self.use_train_aug = use_train_aug\n",
    "        self.train = train\n",
    "        self.mosaic = mosaic\n",
    "        self.square_training = square_training\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # collect image paths\n",
    "        exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.JPG\", \"*.JPEG\", \"*.PNG\", \"*.BMP\"]\n",
    "        self.all_image_paths = []\n",
    "        for e in exts:\n",
    "            self.all_image_paths.extend(glob.glob(os.path.join(images_path, e)))\n",
    "        self.all_image_paths = sorted(set(self.all_image_paths))\n",
    "\n",
    "        # label paths (not directly used, but count for info)\n",
    "        self.all_label_paths = sorted(glob.glob(os.path.join(labels_path, \"*.txt\")))\n",
    "\n",
    "        print(f\"Image count: {len(self.all_image_paths)}\")\n",
    "        print(f\"Label count: {len(self.all_label_paths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    # 1. Load and preprocess image\n",
    "        img_path = self.all_image_paths[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.img_size)\n",
    "        image = image.astype('float32') / 255.0  # normalize to [0,1]\n",
    "\n",
    "        # 2. Load YOLO-format labels\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        lbl_path = os.path.join(self.labels_path, base + \".txt\")\n",
    "        yolo = []\n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, xc, yc, w, h = map(float, line.split())\n",
    "                    yolo.append([cls, xc, yc, w, h])\n",
    "        yolo = torch.tensor(yolo, dtype=torch.float32) if yolo else torch.zeros((0, 5), dtype=torch.float32)\n",
    "\n",
    "        # 3. Convert YOLO boxes to [x1,y1,x2,y2,label] in pixels\n",
    "        boxes = []\n",
    "        for cls, xc, yc, w, h in yolo:\n",
    "            x1 = (xc - w/2) * self.img_size[0]\n",
    "            y1 = (yc - h/2) * self.img_size[1]\n",
    "            x2 = (xc + w/2) * self.img_size[0]\n",
    "            y2 = (yc + h/2) * self.img_size[1]\n",
    "            boxes.append([x1, y1, x2, y2, int(cls)])\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        if boxes.ndim == 1:\n",
    "            boxes = boxes.unsqueeze(0)  # single box ‚Üí (1,5)\n",
    "\n",
    "        # 4. Clamp & normalize box coordinates\n",
    "        if boxes.numel() > 0:\n",
    "            # clamp to image bounds\n",
    "            boxes[:, 0].clamp_(0, self.img_size[0])\n",
    "            boxes[:, 1].clamp_(0, self.img_size[1])\n",
    "            boxes[:, 2].clamp_(0, self.img_size[0])\n",
    "            boxes[:, 3].clamp_(0, self.img_size[1])\n",
    "            # normalize to [0,1]\n",
    "            # norm = torch.tensor([self.img_size[0], self.img_size[1], self.img_size[0], self.img_size[1]], dtype=torch.float32)\n",
    "            # boxes[:, :4] /= norm\n",
    "\n",
    "        # 5. Split coords & labels, ensuring definitions even if no boxes\n",
    "        if boxes.numel() > 0:\n",
    "            labels = boxes[:, 4].long()\n",
    "            coords = boxes[:, :4]\n",
    "        else:\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            coords = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        # 6. Apply transforms (e.g., Albumentations) to both image & boxes\n",
    "        if self.transforms:\n",
    "            data = self.transforms(image=image, bboxes=coords.tolist(), labels=labels.tolist())\n",
    "            image = data['image']\n",
    "            b, l = data['bboxes'], data['labels']\n",
    "            if b:\n",
    "                coords = torch.tensor(b, dtype=torch.float32)\n",
    "                labels = torch.tensor(l, dtype=torch.int64)\n",
    "            else:\n",
    "                coords = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            # If no transforms, convert image to tensor [C,H,W]\n",
    "            image = torch.tensor(image).permute(2, 0, 1)\n",
    "\n",
    "        # 7. Build target dict\n",
    "        target = {\n",
    "            'boxes': coords,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c15a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomDataset(\n",
    "#     images_path=VAL_IMAGES,\n",
    "#     labels_path=VAL_LABELS,\n",
    "#     img_size=(640, 640),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89409991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomDataset(\n",
    "#     images_path=VAL_IMAGES,\n",
    "#     labels_path=VAL_LABELS,\n",
    "#     img_size=(640, 640)\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=2,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,   # try 0 first for debugging\n",
    "#     collate_fn=collate_fn\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed243016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_loader):\n",
    "#     print(f\"Batch {i}: OK\")\n",
    "#     if i == 1:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7d35d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2):\n",
    "#     img, target = dataset[i]\n",
    "#     print(\"Image shape:\", img.shape)\n",
    "#     print(\"Boxes:\", target[\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef557a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2fa0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Sampler\n",
    "from typing import Optional\n",
    "\n",
    "def create_train_dataset(\n",
    "    train_dir_images,\n",
    "    train_dir_labels,\n",
    "    img_size,\n",
    "    classes,\n",
    "    use_train_aug=False,\n",
    "    mosaic=1.0,\n",
    "    square_training=False,\n",
    "):\n",
    "    train_dataset = CustomDataset(\n",
    "        train_dir_images,\n",
    "        train_dir_labels,\n",
    "        img_size,\n",
    "        classes,\n",
    "        get_train_transform(),\n",
    "        use_train_aug=use_train_aug,\n",
    "        train=True,\n",
    "        mosaic=mosaic,\n",
    "        square_training=square_training,\n",
    "    )\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def create_valid_dataset(valid_dir_images, valid_dir_labels, img_size, classes, square_training=False):\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_dir_images,\n",
    "        valid_dir_labels,\n",
    "        img_size,\n",
    "        classes,\n",
    "        get_valid_transform(),\n",
    "        train=False,\n",
    "        square_training=square_training,\n",
    "    )\n",
    "    return valid_dataset\n",
    "\n",
    "\n",
    "def create_train_loader(train_dataset, batch_size, num_workers=0, batch_sampler=None):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        sampler=batch_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def create_valid_loader(valid_dataset, batch_size, num_workers=0, batch_sampler=None):\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        sampler=batch_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return valid_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f58314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hariom Meena\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\albumentations\\core\\composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 3488\n",
      "Label count: 3488\n",
      "Image count: 1335\n",
      "Label count: 1335\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_train_dataset(\n",
    "    TRAIN_IMAGES,\n",
    "    TRAIN_LABELS,\n",
    "    (640, 640),\n",
    "    classes,\n",
    "    use_train_aug=True,\n",
    "    mosaic=1.0,\n",
    ")\n",
    "\n",
    "valid_dataset = create_valid_dataset(\n",
    "    VAL_IMAGES,\n",
    "    VAL_LABELS,\n",
    "    (640, 640),\n",
    "    classes,\n",
    ")\n",
    "    \n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "# valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "# train_loader = create_train_loader(train_dataset, batch_size=6, num_workers=0, batch_sampler=train_sampler)\n",
    "# valid_loader = create_valid_loader(valid_dataset, batch_size=6, num_workers=0, batch_sampler=valid_sampler)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "train_loader = create_train_loader(\n",
    "    train_dataset,\n",
    "    batch_size=6,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=0,       # bump this if you can\n",
    ")\n",
    "\n",
    "valid_loader = create_valid_loader(\n",
    "    valid_dataset,\n",
    "    batch_size=6,\n",
    "    batch_sampler=valid_sampler,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac30f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch loaded!\n"
     ]
    }
   ],
   "source": [
    "for images, targets in train_loader:\n",
    "    print(\"First batch loaded!\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ed688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a5231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38843944",
   "metadata": {},
   "source": [
    "## Faster R-CNN Model\n",
    "\n",
    "In this step, we will create the Faster R-CNN model using the PyTorch library. We will use a pre-trained ResNet-101 backbone and replace the classification and regression heads with new heads for our dataset. We will also define the loss function and optimizer for training the model.\n",
    "\n",
    "RCNN model is defined with the following components:\n",
    "- **Backbone**: A pre-trained ResNet-101 backbone is used to extract features from the input images.\n",
    "- **Region Proposal Network (RPN)**: A region proposal network is used to generate region proposals for objects in the images.\n",
    "- **RoI Pooling**: RoI pooling is used to extract features from the region proposals and resize them to a fixed size.\n",
    "- **Head**: The head of the model consists of two subnetworks: a classification subnetwork that predicts the class of the object in the region proposal and a regression subnetwork that predicts the bounding box coordinates of the object.\n",
    "- **Loss Function**: The loss function used to train the model is a combination of the classification and regression losses.\n",
    "- **Optimizer**: The SGD optimizer is used to optimize the model parameters during training.\n",
    "- **Learning Rate Scheduler**: A learning rate scheduler is used to adjust the learning rate during training.\n",
    "\n",
    "The important difference here is that we are using GIOU instead of the default IOU loss. GIOU is a loss function used to measure the similarity between two bounding boxes. This loss function can help the model learn to predict more accurate bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d12a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GIOU(boxes1, boxes2):\n",
    "    EPSILON = 1e-6\n",
    "    boxes1 = torch.cat((boxes1[..., :2] - boxes1[..., 2:] / 2, boxes1[..., :2] + boxes1[..., 2:] / 2), 1)\n",
    "    boxes2 = torch.cat((boxes2[..., :2] - boxes2[..., 2:] / 2, boxes2[..., :2] + boxes2[..., 2:] / 2), 1)\n",
    "\n",
    "    inter_left_up = torch.max(boxes1[..., :2], boxes2[..., :2])\n",
    "    inter_right_down = torch.min(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    inter_section = torch.max(inter_right_down - inter_left_up, torch.tensor(0.0).to(device))\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "\n",
    "    area1 = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
    "    area2 = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
    "\n",
    "    union_area = area1 + area2 - inter_area\n",
    "\n",
    "    iou = inter_area / (union_area + EPSILON)\n",
    "\n",
    "    enclose_left_up = torch.min(boxes1[..., :2], boxes2[..., :2])\n",
    "    enclose_right_down = torch.max(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    enclose_section = torch.max(enclose_right_down - enclose_left_up, torch.tensor(0.0).to(device))\n",
    "    enclose_area = enclose_section[..., 0] * enclose_section[..., 1]\n",
    "\n",
    "    giou = iou - 1.0 * (enclose_area - union_area) / (enclose_area + EPSILON)\n",
    "\n",
    "    return giou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e948452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=4):\n",
    "    model_backbone = torchvision.models.resnet101(weights=\"DEFAULT\")\n",
    "\n",
    "    conv1 = model_backbone.conv1\n",
    "    bn1 = model_backbone.bn1\n",
    "    relu = model_backbone.relu\n",
    "    max_pool = model_backbone.maxpool\n",
    "    layer1 = model_backbone.layer1\n",
    "    layer2 = model_backbone.layer2\n",
    "    layer3 = model_backbone.layer3\n",
    "    layer4 = model_backbone.layer4\n",
    "\n",
    "    backbone = nn.Sequential(conv1, bn1, relu, max_pool, layer1, layer2, layer3, layer4)\n",
    "    backbone.out_channels = 2048\n",
    "\n",
    "    # Here, we are using 5x3 anchors.\n",
    "    # Meaning, anchors with 5 different sizes and 3 different aspect ratios.\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128),), aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    roi_pooler = ops.MultiScaleRoIAlign(\n",
    "        featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        rpn_pre_nms_top_n_train=2000,\n",
    "        rpn_pre_nms_top_n_test=1000,\n",
    "        rpn_post_nms_top_n_train=2000,\n",
    "        rpn_post_nms_top_n_test=1000,\n",
    "        rpn_nms_thresh=0.7,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34167bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2991118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(2048, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(2048, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(num_classes=4)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc78e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'fasterrcnn_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "451e1d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 582\n",
      "Valid batches: 223\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Valid batches: {len(valid_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b35e192f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_box' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m fig, axs = plt.subplots(nrows=\u001b[32m4\u001b[39m, ncols=\u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m15\u001b[39m))\n\u001b[32m      3\u001b[39m plt.subplot(\u001b[32m4\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m plt.plot(\u001b[43mavg_box\u001b[49m)\n\u001b[32m      5\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mTrain Box Loss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m plt.subplot(\u001b[32m4\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'avg_box' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 15))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(avg_box)\n",
    "plt.title(\"Train Box Loss\")\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.plot(train_class_loss)\n",
    "plt.title(\"Train Class Loss\")\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.plot(valid_precision)\n",
    "plt.title(\"Validation Precision\")\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "plt.plot(valid_recall)\n",
    "plt.title(\"Validation Recall\")\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "plt.plot(valid_mAP50)\n",
    "plt.title(\"Validation mAP50\")\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "plt.plot(valid_mAP50_95)\n",
    "plt.title(\"Validation mAP50-95\")\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "plt.plot(valid_box_loss)\n",
    "plt.title(\"Validation Box Loss\")\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "plt.plot(valid_class_loss)\n",
    "plt.title(\"Validation Class Loss\")\n",
    "\n",
    "plt.suptitle(\"Training Metrics and Loss\", fontsize=24)\n",
    "plt.subplots_adjust(top=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddba8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# train_box_loss = []\n",
    "# train_class_loss = []\n",
    "# valid_precision = []\n",
    "# valid_recall = []\n",
    "# valid_mAP50 = []\n",
    "# valid_mAP50_95 = []\n",
    "# valid_box_loss = []\n",
    "# valid_class_loss = []\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_train_box_loss = 0\n",
    "#     total_train_class_loss = 0\n",
    "\n",
    "#     train_loader_progress = tqdm(train_loader, desc=f\"[Epoch {epoch+1}/10] Training\", position=0, leave=True, dynamic_ncols=True, smoothing=0.3)\n",
    "#     for images, targets in train_loader_progress:\n",
    "#         images = [image.to(device) for image in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         outputs = model(images, targets)\n",
    "#         loss_dict = outputs\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         box_loss = loss_dict[\"loss_box_reg\"].item()\n",
    "#         cls_loss = loss_dict[\"loss_classifier\"].item()\n",
    "\n",
    "#         train_box_loss.append(box_loss)\n",
    "#         train_class_loss.append(cls_loss)\n",
    "\n",
    "#         total_train_box_loss += box_loss\n",
    "#         total_train_class_loss += cls_loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loader_progress.set_postfix({\n",
    "#             'BoxLoss': f'{box_loss:.4f}',\n",
    "#             'ClsLoss': f'{cls_loss:.4f}'\n",
    "#         })\n",
    "\n",
    "#     model.eval()\n",
    "#     total_valid_box_loss = 0\n",
    "#     total_valid_class_loss = 0\n",
    "\n",
    "#     valid_loader_progress = tqdm(valid_loader, desc=f\"[Epoch {epoch+1}/10] Validation\", position=0, leave=True, dynamic_ncols=True, smoothing=0.3)\n",
    "#     for images, targets in valid_loader_progress:\n",
    "#         images = [image.to(device) for image in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         predicted_boxes = loss_dict[\"boxes\"]\n",
    "#         target_boxes = [t[\"boxes\"] for t in targets]\n",
    "#         giou_loss = GIOU(predicted_boxes, target_boxes)\n",
    "#         losses += giou_loss\n",
    "\n",
    "#         box_loss = loss_dict[\"loss_box_reg\"].item()\n",
    "#         cls_loss = loss_dict[\"loss_classifier\"].item()\n",
    "\n",
    "#         valid_box_loss.append(box_loss)\n",
    "#         valid_class_loss.append(cls_loss)\n",
    "\n",
    "#         total_valid_box_loss += box_loss\n",
    "#         total_valid_class_loss += cls_loss\n",
    "\n",
    "#         valid_loader_progress.set_postfix({\n",
    "#             'BoxLoss': f'{box_loss:.4f}',\n",
    "#             'ClsLoss': f'{cls_loss:.4f}'\n",
    "#         })\n",
    "\n",
    "#         precision = []\n",
    "#         recall = []\n",
    "#         AP50 = []\n",
    "#         AP50_95 = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "\n",
    "#         for i in range(len(targets)):\n",
    "#             target = targets[i]\n",
    "#             output = outputs[i]\n",
    "\n",
    "#             pred_boxes = output[\"boxes\"]\n",
    "#             pred_scores = output[\"scores\"]\n",
    "#             pred_labels = output[\"labels\"]\n",
    "\n",
    "#             true_boxes = target[\"boxes\"]\n",
    "#             true_labels = target[\"labels\"]\n",
    "\n",
    "#             for label in range(4):\n",
    "#                 true_boxes_label = true_boxes[true_labels == label]\n",
    "#                 pred_boxes_label = pred_boxes[pred_labels == label]\n",
    "#                 pred_scores_label = pred_scores[pred_labels == label]\n",
    "\n",
    "#                 if len(true_boxes_label) == 0 or len(pred_boxes_label) == 0:\n",
    "#                     precision.append(0)\n",
    "#                     recall.append(0)\n",
    "#                     AP50.append(0)\n",
    "#                     AP50_95.append(0)\n",
    "#                     continue\n",
    "\n",
    "#                 iou = GIOU(true_boxes_label, pred_boxes_label)\n",
    "#                 iou_thresholds = torch.linspace(0.5, 0.95, 10).to(device)\n",
    "\n",
    "#                 true_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_negatives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "\n",
    "#                 for j, iou_threshold in enumerate(iou_thresholds):\n",
    "#                     true_positives[j] = torch.sum(iou > iou_threshold)\n",
    "#                     false_positives[j] = torch.sum(iou <= iou_threshold)\n",
    "#                     false_negatives[j] = len(true_boxes_label) - true_positives[j]\n",
    "\n",
    "#                 prec = true_positives / (true_positives + false_positives + 1e-6)\n",
    "#                 rec = true_positives / (true_positives + false_negatives + 1e-6)\n",
    "\n",
    "#                 precision.append(prec)\n",
    "#                 recall.append(rec)\n",
    "#                 AP50.append((prec * rec).mean())\n",
    "#                 AP50_95.append((prec * rec).mean())\n",
    "\n",
    "#         valid_precision.append(precision[-1])\n",
    "#         valid_recall.append(recall[-1])\n",
    "#         valid_mAP50.append(AP50[-1])\n",
    "#         valid_mAP50_95.append(AP50_95[-1])\n",
    "\n",
    "#     tqdm.write(f\"Epoch {epoch+1}/10\")\n",
    "#     tqdm.write(f\"Train Box Loss: {total_train_box_loss:.4f}, Class Loss: {total_train_class_loss:.4f}\")\n",
    "#     tqdm.write(f\"Valid Box Loss: {total_valid_box_loss:.4f}, Class Loss: {total_valid_class_loss:.4f}\")\n",
    "#     if valid_mAP50:\n",
    "#         tqdm.write(f\"Valid mAP@0.5: {valid_mAP50[-1]:.4f}, mAP@0.5:0.95: {valid_mAP50_95[-1]:.4f}\")\n",
    "#     tqdm.write(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22ee386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hariom Meena\\AppData\\Local\\Temp\\ipykernel_22052\\1973135367.py:128: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/10 [Train]:   0%|          | 0/582 [00:00<?, ?batch/s]C:\\Users\\Hariom Meena\\AppData\\Local\\Temp\\ipykernel_22052\\1973135367.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [1:33:22<00:00,  9.63s/batch, Box=0.146, Cls=0.107]\n",
      "Epoch 1/10 [Val-Loss]:   0%|          | 0/223 [00:00<?, ?batch/s]C:\\Users\\Hariom Meena\\AppData\\Local\\Temp\\ipykernel_22052\\1973135367.py:172: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [15:25<00:00,  4.15s/batch, VBox=0.129, VCls=0.086]\n",
      "Epoch 1/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [15:43<00:00,  4.23s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1148, Cls Loss: 0.1511\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1610, Cls Loss: 0.1315\n",
      " ‚ñ∂ mAP@0.50: 0.2116, mAP@0.50:0.95: 0.0861\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:56:57<00:00, 18.24s/batch, Box=0.166, Cls=0.073]  \n",
      "Epoch 2/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:32<00:00,  3.10s/batch, VBox=0.109, VCls=0.074]\n",
      "Epoch 2/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [59:59<00:00, 16.14s/batch]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1552, Cls Loss: 0.1041\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1558, Cls Loss: 0.1000\n",
      " ‚ñ∂ mAP@0.50: 0.3347, mAP@0.50:0.95: 0.1681\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:53:33<00:00, 17.89s/batch, Box=0.122, Cls=0.043]  \n",
      "Epoch 3/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:07<00:00,  3.00s/batch, VBox=0.126, VCls=0.078]\n",
      "Epoch 3/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [26:31<00:00,  7.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1533, Cls Loss: 0.0920\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1527, Cls Loss: 0.0895\n",
      " ‚ñ∂ mAP@0.50: 0.3938, mAP@0.50:0.95: 0.2132\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:53:10<00:00, 17.85s/batch, Box=0.148, Cls=0.072]  \n",
      "Epoch 4/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:08<00:00,  3.00s/batch, VBox=0.149, VCls=0.063]\n",
      "Epoch 4/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [26:07<00:00,  7.03s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1431, Cls Loss: 0.0828\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1488, Cls Loss: 0.0869\n",
      " ‚ñ∂ mAP@0.50: 0.4232, mAP@0.50:0.95: 0.2336\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:53:18<00:00, 17.87s/batch, Box=0.245, Cls=0.165]  \n",
      "Epoch 5/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:12<00:00,  3.02s/batch, VBox=0.136, VCls=0.062]\n",
      "Epoch 5/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [26:29<00:00,  7.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1377, Cls Loss: 0.0767\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1445, Cls Loss: 0.0820\n",
      " ‚ñ∂ mAP@0.50: 0.4568, mAP@0.50:0.95: 0.2564\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:52:52<00:00, 17.82s/batch, Box=0.090, Cls=0.068]  \n",
      "Epoch 6/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:15<00:00,  3.03s/batch, VBox=0.116, VCls=0.057]\n",
      "Epoch 6/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [25:52<00:00,  6.96s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1298, Cls Loss: 0.0714\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1340, Cls Loss: 0.0790\n",
      " ‚ñ∂ mAP@0.50: 0.4828, mAP@0.50:0.95: 0.2864\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:52:50<00:00, 17.82s/batch, Box=0.033, Cls=0.061]  \n",
      "Epoch 7/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:13<00:00,  3.02s/batch, VBox=0.119, VCls=0.063]\n",
      "Epoch 7/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [26:11<00:00,  7.05s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1227, Cls Loss: 0.0664\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1506, Cls Loss: 0.0753\n",
      " ‚ñ∂ mAP@0.50: 0.4976, mAP@0.50:0.95: 0.2836\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:52:45<00:00, 17.81s/batch, Box=0.128, Cls=0.041]  \n",
      "Epoch 8/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:15<00:00,  3.03s/batch, VBox=0.114, VCls=0.073]\n",
      "Epoch 8/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [25:52<00:00,  6.96s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1167, Cls Loss: 0.0613\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1305, Cls Loss: 0.0727\n",
      " ‚ñ∂ mAP@0.50: 0.5087, mAP@0.50:0.95: 0.3067\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:52:32<00:00, 17.79s/batch, Box=0.148, Cls=0.067]  \n",
      "Epoch 9/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:03<00:00,  2.98s/batch, VBox=0.110, VCls=0.059]\n",
      "Epoch 9/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [25:34<00:00,  6.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1106, Cls Loss: 0.0580\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1199, Cls Loss: 0.0699\n",
      " ‚ñ∂ mAP@0.50: 0.5258, mAP@0.50:0.95: 0.3239\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 582/582 [2:52:42<00:00, 17.80s/batch, Box=0.094, Cls=0.045]  \n",
      "Epoch 10/10 [Val-Loss]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [11:17<00:00,  3.04s/batch, VBox=0.106, VCls=0.050]\n",
      "Epoch 10/10 [Val-mAP]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [25:39<00:00,  6.90s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10 summary:\n",
      " ‚ñ∂ Train   Box Loss: 0.1072, Cls Loss: 0.0548\n",
      " ‚ñ∂ Val-Loss Box Loss: 0.1244, Cls Loss: 0.0736\n",
      " ‚ñ∂ mAP@0.50: 0.5313, mAP@0.50:0.95: 0.3206\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def train_and_validate(\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_loader,\n",
    "#     valid_loader,\n",
    "#     device,\n",
    "#     num_epochs=10,\n",
    "# ):\n",
    "#     scaler = GradScaler()\n",
    "#     metric = MeanAveragePrecision()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî TRAIN ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "#         model.train()\n",
    "#         running_box_loss = 0.0\n",
    "#         running_cls_loss = 0.0\n",
    "\n",
    "#         train_bar = tqdm(\n",
    "#             train_loader,\n",
    "#             desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\",\n",
    "#             unit=\"batch\",\n",
    "#             leave=False,\n",
    "#         )\n",
    "#         for images, targets in train_bar:\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             with autocast():\n",
    "#                 loss_dict = model(images, targets)\n",
    "#                 # guard: make sure we actually got a dict\n",
    "#                 if not isinstance(loss_dict, dict):\n",
    "#                     raise RuntimeError(\n",
    "#                         f\"Expected a dict of losses, but got {type(loss_dict)}. \"\n",
    "#                         \"Check that you always pass valid targets.\"\n",
    "#                     )\n",
    "#                 loss = sum(loss_dict.values())\n",
    "\n",
    "#             # backward + step\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "\n",
    "#             box_l = loss_dict[\"loss_box_reg\"].item()\n",
    "#             cls_l = loss_dict[\"loss_classifier\"].item()\n",
    "#             running_box_loss += box_l\n",
    "#             running_cls_loss += cls_l\n",
    "\n",
    "#             train_bar.set_postfix(Box=f\"{box_l:.3f}\", Cls=f\"{cls_l:.3f}\")\n",
    "\n",
    "#         avg_train_box = running_box_loss / len(train_loader)\n",
    "#         avg_train_cls = running_cls_loss / len(train_loader)\n",
    "\n",
    "#         # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî VALIDATION ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "#         model.eval()\n",
    "#         metric.reset()\n",
    "#         running_val_box = 0.0\n",
    "#         running_val_cls = 0.0\n",
    "\n",
    "#         valid_bar = tqdm(\n",
    "#             valid_loader,\n",
    "#             desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\",\n",
    "#             unit=\"batch\",\n",
    "#             leave=False,\n",
    "#         )\n",
    "#         with torch.no_grad():\n",
    "#             for images, targets in valid_bar:\n",
    "#                 images = [img.to(device) for img in images]\n",
    "#                 targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#                 # first forward: get losses\n",
    "#                 with autocast():\n",
    "#                     loss_dict = model(images, targets)\n",
    "#                     if not isinstance(loss_dict, dict):\n",
    "#                         raise RuntimeError(\n",
    "#                             f\"Expected a dict of losses, but got {type(loss_dict)}. \"\n",
    "#                             \"Check that you always pass valid targets.\"\n",
    "#                         )\n",
    "\n",
    "#                 box_l = loss_dict[\"loss_box_reg\"].item()\n",
    "#                 cls_l = loss_dict[\"loss_classifier\"].item()\n",
    "#                 running_val_box += box_l\n",
    "#                 running_val_cls += cls_l\n",
    "\n",
    "#                 # second forward: get predictions for mAP\n",
    "#                 with autocast():\n",
    "#                     preds = model(images)\n",
    "\n",
    "#                 metric.update(preds, targets)\n",
    "\n",
    "#                 valid_bar.set_postfix(Box=f\"{box_l:.3f}\", Cls=f\"{cls_l:.3f}\")\n",
    "\n",
    "#         avg_val_box = running_val_box / len(valid_loader)\n",
    "#         avg_val_cls = running_val_cls / len(valid_loader)\n",
    "#         stats = metric.compute()  # {'map_50': ..., 'map': ..., ...}\n",
    "\n",
    "#         # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî EPOCH SUMMARY ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#         print(f\" ‚ñ∂ Train Box Loss: {avg_train_box:.4f}, Cls Loss: {avg_train_cls:.4f}\")\n",
    "#         print(f\" ‚ñ∂ Val   Box Loss: {avg_val_box:.4f}, Cls Loss: {avg_val_cls:.4f}\")\n",
    "#         print(f\" ‚ñ∂ mAP@0.50: {stats['map_50']:.4f}, mAP@0.50:0.95: {stats['map']:.4f}\")\n",
    "#         print(\"-\" * 60)\n",
    "# train_and_validate(model, optimizer, train_loader, valid_loader, device, num_epochs=10)\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_and_validate(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "):\n",
    "    scaler = GradScaler()\n",
    "    metric = MeanAveragePrecision()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî TRAIN ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "        model.train()\n",
    "        running_box = running_cls = 0.0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", unit=\"batch\")\n",
    "        \n",
    "        for images, targets in train_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                loss_dict = model(images, targets)\n",
    "                if not isinstance(loss_dict, dict):\n",
    "                    raise RuntimeError(f\"Expected loss dict in train, got {type(loss_dict)}\")\n",
    "                loss = sum(loss_dict.values())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            box_l = loss_dict[\"loss_box_reg\"].item()\n",
    "            cls_l = loss_dict[\"loss_classifier\"].item()\n",
    "            running_box += box_l\n",
    "            running_cls += cls_l\n",
    "\n",
    "            train_bar.set_postfix(Box=f\"{box_l:.3f}\", Cls=f\"{cls_l:.3f}\")\n",
    "\n",
    "        avg_box = running_box / len(train_loader)\n",
    "        avg_cls = running_cls / len(train_loader)\n",
    "\n",
    "        # ‚Äî‚Äî‚Äî VALIDATION LOSS ‚Äî‚Äî‚Äî\n",
    "        # We need the model in train mode so model(images, targets) returns losses\n",
    "        model.train()\n",
    "        running_vbox = running_vcls = 0.0\n",
    "        val_loss_bar = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val-Loss]\", unit=\"batch\")\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loss_bar:\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                with autocast():\n",
    "                    vloss_dict = model(images, targets)\n",
    "                    if not isinstance(vloss_dict, dict):\n",
    "                        raise RuntimeError(f\"Expected loss dict in val-loss, got {type(vloss_dict)}\")\n",
    "\n",
    "                vbox_l = vloss_dict[\"loss_box_reg\"].item()\n",
    "                vcls_l = vloss_dict[\"loss_classifier\"].item()\n",
    "                running_vbox += vbox_l\n",
    "                running_vcls += vcls_l\n",
    "\n",
    "                val_loss_bar.set_postfix(VBox=f\"{vbox_l:.3f}\", VCls=f\"{vcls_l:.3f}\")\n",
    "\n",
    "        avg_vbox = running_vbox / len(valid_loader)\n",
    "        avg_vcls = running_vcls / len(valid_loader)\n",
    "\n",
    "        # ‚Äî‚Äî‚Äî VALIDATION METRICS (mAP) ‚Äî‚Äî‚Äî\n",
    "        model.eval()\n",
    "        metric.reset()\n",
    "        val_map_bar = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val-mAP]\", unit=\"batch\")\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_map_bar:\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                preds = model(images)  # now returns a list of preds\n",
    "                metric.update(preds, targets)\n",
    "\n",
    "        stats = metric.compute()  # e.g. {'map_50': ..., 'map': ...}\n",
    "\n",
    "        # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî EPOCH SUMMARY ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} summary:\")\n",
    "        print(f\" ‚ñ∂ Train   Box Loss: {avg_box:.4f}, Cls Loss: {avg_cls:.4f}\")\n",
    "        print(f\" ‚ñ∂ Val-Loss Box Loss: {avg_vbox:.4f}, Cls Loss: {avg_vcls:.4f}\")\n",
    "        print(f\" ‚ñ∂ mAP@0.50: {stats['map_50']:.4f}, mAP@0.50:0.95: {stats['map']:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "train_and_validate(model, optimizer, train_loader, valid_loader, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cb1e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the model weights\n",
    "torch.save(model.state_dict(), 'fasterrcnn_model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42854483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists('fasterrcnn_model_weights.pth'))  # should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e01e35a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 1111\n",
      "Label count: 1111\n"
     ]
    }
   ],
   "source": [
    "test_dataset = create_valid_dataset(\n",
    "    TEST_IMAGES,\n",
    "    TEST_LABELS,\n",
    "    (640, 640),\n",
    "    classes,\n",
    "    \n",
    ")\n",
    "\n",
    "test_loader = create_valid_loader(test_dataset, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "249c8721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "map: 0.3075\n",
      "map_50: 0.5057\n",
      "map_75: 0.3297\n",
      "map_small: 0.1026\n",
      "map_medium: 0.3579\n",
      "map_large: 0.4190\n",
      "mar_1: 0.1115\n",
      "mar_10: 0.3414\n",
      "mar_100: 0.3825\n",
      "mar_small: 0.1442\n",
      "mar_medium: 0.4402\n",
      "mar_large: 0.4825\n",
      "map_per_class: [0.         0.54568785 0.19358154 0.49092668]\n",
      "mar_100_per_class: [0.         0.6057631  0.3626728  0.56138617]\n",
      "classes: [0 1 2 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m             f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Plot Precision-Recall curve for each class\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m precision, recall, _ = \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Plot the Precision-Recall curve\u001b[39;00m\n\u001b[32m     70\u001b[39m plt.figure()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:1005\u001b[39m, in \u001b[36mprecision_recall_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight, drop_intermediate, probas_pred)\u001b[39m\n\u001b[32m    996\u001b[39m     warnings.warn(\n\u001b[32m    997\u001b[39m         (\n\u001b[32m    998\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprobas_pred was deprecated in version 1.5 and will be removed in 1.7.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1001\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1002\u001b[39m     )\n\u001b[32m   1003\u001b[39m     y_score = probas_pred\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m fps, tps, thresholds = \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) > \u001b[32m2\u001b[39m:\n\u001b[32m   1010\u001b[39m     \u001b[38;5;66;03m# Drop thresholds corresponding to points where true positives (tps)\u001b[39;00m\n\u001b[32m   1011\u001b[39m     \u001b[38;5;66;03m# do not change from the previous or subsequent point. This will keep\u001b[39;00m\n\u001b[32m   1012\u001b[39m     \u001b[38;5;66;03m# only the first and last point for each tps value. All points\u001b[39;00m\n\u001b[32m   1013\u001b[39m     \u001b[38;5;66;03m# with the same tps value have the same recall and thus x coordinate.\u001b[39;00m\n\u001b[32m   1014\u001b[39m     \u001b[38;5;66;03m# They appear as a vertical line on the plot.\u001b[39;00m\n\u001b[32m   1015\u001b[39m     optimal_idxs = np.where(\n\u001b[32m   1016\u001b[39m         np.concatenate(\n\u001b[32m   1017\u001b[39m             [[\u001b[38;5;28;01mTrue\u001b[39;00m], np.logical_or(np.diff(tps[:-\u001b[32m1\u001b[39m]), np.diff(tps[\u001b[32m1\u001b[39m:])), [\u001b[38;5;28;01mTrue\u001b[39;00m]]\n\u001b[32m   1018\u001b[39m         )\n\u001b[32m   1019\u001b[39m     )[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:818\u001b[39m, in \u001b[36m_binary_clf_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight)\u001b[39m\n\u001b[32m    816\u001b[39m y_type = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m format is not supported\u001b[39m\u001b[33m\"\u001b[39m.format(y_type))\n\u001b[32m    820\u001b[39m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[32m    821\u001b[39m y_true = column_or_1d(y_true)\n",
      "\u001b[31mValueError\u001b[39m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# Evaluation directory\n",
    "OUTPUT_DIR = \"evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Move model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize metric calculator\n",
    "metric = MeanAveragePrecision(class_metrics=True)\n",
    "\n",
    "# Store metrics over dataset\n",
    "CONFIDENCE_THRESHOLD = 0.5  # Set your desired threshold\n",
    "num = 0\n",
    "\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "\n",
    "for images, targets in test_loader:\n",
    "    images = list(img.to(device) for img in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    # Update mAP metric\n",
    "    metric.update(outputs, targets)\n",
    "\n",
    "    # Collect labels and scores for Precision-Recall\n",
    "    for i in range(len(images)):\n",
    "        for label, score in zip(outputs[i][\"labels\"], outputs[i][\"scores\"]):\n",
    "            if score >= CONFIDENCE_THRESHOLD:\n",
    "                all_labels.append(label.item())\n",
    "                all_scores.append(score.item())\n",
    "\n",
    "# Compute metrics\n",
    "metrics = metric.compute()\n",
    "\n",
    "# Print metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        if v.numel() == 1:\n",
    "            print(f\"{k}: {v.item():.4f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v.cpu().numpy()}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# Save metrics to file\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.txt\"), \"w\") as f:\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            if v.numel() == 1:\n",
    "                f.write(f\"{k}: {v.item():.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{k}: {v.cpu().numpy()}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='b', label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the Precision-Recall curve plot\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"precision_recall_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# OPTIONAL: Plot Precision-Recall curve per class (if needed)\n",
    "if \"map_per_class\" in metrics:\n",
    "    pr_per_class = metrics[\"map_per_class\"]\n",
    "    if pr_per_class is not None and isinstance(pr_per_class, torch.Tensor):\n",
    "        plt.plot(pr_per_class.cpu().numpy(), label=\"mAP per class\")\n",
    "        plt.xlabel(\"Class ID\")\n",
    "        plt.ylabel(\"mAP\")\n",
    "        plt.title(\"mAP per class\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"map_per_class.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34286038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mAP@0.5 per class:\n",
      "Class 0: 0.0000\n",
      "Class 1: 0.7988\n",
      "Class 2: 0.4023\n",
      "Class 3: 0.8219\n",
      "\n",
      "mAP@0.5:0.95 per class:\n",
      "Class 0: 0.0000\n",
      "Class 1: 0.5457\n",
      "Class 2: 0.1936\n",
      "Class 3: 0.4909\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Setup ‚Äî‚Äî‚Äî\n",
    "OUTPUT_DIR = \"evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model.eval()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Two metrics: one at IoU=0.5, one at default [0.5:0.95] ‚Äî‚Äî‚Äî\n",
    "metric50   = MeanAveragePrecision(class_metrics=True,   iou_thresholds=[0.5])\n",
    "metric5095 = MeanAveragePrecision(class_metrics=True)   # defaults to [0.5,0.55,‚Ä¶,0.95]\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Run evaluation ‚Äî‚Äî‚Äî\n",
    "for images, targets in test_loader:\n",
    "    images = [img.to(device) for img in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    metric50.update(outputs, targets)\n",
    "    metric5095.update(outputs, targets)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Compute metrics ‚Äî‚Äî‚Äî\n",
    "metrics50   = metric50.compute()    # only IoU=0.5\n",
    "metrics5095 = metric5095.compute()  # IoU=0.5‚Üí0.95 averaged\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Extract per-class tensors ‚Äî‚Äî‚Äî\n",
    "map50_pc   = metrics50[\"map_per_class\"]   # shape [num_classes]\n",
    "map5095_pc = metrics5095[\"map_per_class\"] # shape [num_classes]\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Print & Save full raw metrics for reference ‚Äî‚Äî‚Äî\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_full.txt\"), \"w\") as f:\n",
    "    for label, m in [(\"mAP@0.5\", metrics50), (\"mAP@0.5:0.95\", metrics5095)]:\n",
    "        f.write(f\"--- {label} ---\\n\")\n",
    "        for k, v in m.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                val = v.item() if v.numel()==1 else v.cpu().numpy().tolist()\n",
    "            else:\n",
    "                val = v\n",
    "            f.write(f\"{k}: {val}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Print & Save per-class results ‚Äî‚Äî‚Äî\n",
    "with open(os.path.join(OUTPUT_DIR, \"mAP_per_class.txt\"), \"w\") as f:\n",
    "    print(\"\\nmAP@0.5 per class:\")\n",
    "    f.write(\"mAP@0.5 per class:\\n\")\n",
    "    for cid, score in enumerate(map50_pc):\n",
    "        line = f\"Class {cid}: {score.item():.4f}\"\n",
    "        print(line); f.write(line + \"\\n\")\n",
    "\n",
    "    print(\"\\nmAP@0.5:0.95 per class:\")\n",
    "    f.write(\"\\nmAP@0.5:0.95 per class:\\n\")\n",
    "    for cid, score in enumerate(map5095_pc):\n",
    "        line = f\"Class {cid}: {score.item():.4f}\"\n",
    "        print(line); f.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5cf46838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@0.5 per class:\n",
      "Class 0: 0.0000\n",
      "Class 1: 0.7949\n",
      "Class 2: 0.4317\n",
      "Class 3: 0.7985\n",
      "\n",
      "mAP@0.5:0.95 per class:\n",
      "Class 0: 0.0000\n",
      "Class 1: 0.5486\n",
      "Class 2: 0.1969\n",
      "Class 3: 0.4865\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Setup ‚Äî‚Äî‚Äî\n",
    "OUTPUT_DIR = \"evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model.eval()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Two metrics: IoU=0.5 only, and IoU=0.5‚Üí0.95 avg ‚Äî‚Äî‚Äî\n",
    "metric50   = MeanAveragePrecision(class_metrics=True, iou_thresholds=[0.5])\n",
    "metric5095 = MeanAveragePrecision(class_metrics=True)  # default [0.5,0.55,‚Ä¶,0.95]\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Determine how many batches to run (50% of test_loader) ‚Äî‚Äî‚Äî\n",
    "total_batches = len(test_loader)\n",
    "half_batches  = total_batches // 2\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Run evaluation on just the first half of test_loader ‚Äî‚Äî‚Äî\n",
    "for batch_idx, (images, targets) in enumerate(test_loader):\n",
    "    if batch_idx >= half_batches:\n",
    "        break\n",
    "\n",
    "    images = [img.to(device) for img in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    metric50.update(outputs, targets)\n",
    "    metric5095.update(outputs, targets)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Compute metrics ‚Äî‚Äî‚Äî\n",
    "metrics50   = metric50.compute()    # mAP@0.5\n",
    "metrics5095 = metric5095.compute()  # mAP@[0.5:0.95] avg\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Extract per-class results ‚Äî‚Äî‚Äî\n",
    "map50_pc   = metrics50[\"map_per_class\"]   # 1-D tensor [num_classes]\n",
    "map5095_pc = metrics5095[\"map_per_class\"] # 1-D tensor [num_classes]\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Save full raw metrics ‚Äî‚Äî‚Äî\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_full.txt\"), \"w\") as f:\n",
    "    for label, m in [(\"mAP@0.5\", metrics50), (\"mAP@0.5:0.95\", metrics5095)]:\n",
    "        f.write(f\"--- {label} ---\\n\")\n",
    "        for k, v in m.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                val = v.item() if v.numel()==1 else v.cpu().numpy().tolist()\n",
    "            else:\n",
    "                val = v\n",
    "            f.write(f\"{k}: {val}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Save and print per-class numbers ‚Äî‚Äî‚Äî\n",
    "with open(os.path.join(OUTPUT_DIR, \"mAP_per_class.txt\"), \"w\") as f:\n",
    "    f.write(\"mAP@0.5 per class:\\n\")\n",
    "    for cid, score in enumerate(map50_pc):\n",
    "        f.write(f\"Class {cid}: {score.item():.4f}\\n\")\n",
    "    f.write(\"\\nmAP@0.5:0.95 per class:\\n\")\n",
    "    for cid, score in enumerate(map5095_pc):\n",
    "        f.write(f\"Class {cid}: {score.item():.4f}\\n\")\n",
    "\n",
    "print(\"mAP@0.5 per class:\")\n",
    "for cid, score in enumerate(map50_pc):\n",
    "    print(f\"Class {cid}: {score.item():.4f}\")\n",
    "print(\"\\nmAP@0.5:0.95 per class:\")\n",
    "for cid, score in enumerate(map5095_pc):\n",
    "    print(f\"Class {cid}: {score.item():.4f}\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Plot & save bar chart for mAP@0.5 ‚Äî‚Äî‚Äî\n",
    "classes      = list(range(map50_pc.shape[0]))\n",
    "map50_vals   = [s.item() for s in map50_pc]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(classes, map50_vals)\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"mAP@0.5\")\n",
    "plt.title(\"Per-Class mAP@0.5\")\n",
    "plt.xticks(classes)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"map50_per_class.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Plot & save bar chart for mAP@0.5:0.95 ‚Äî‚Äî‚Äî\n",
    "map5095_vals = [s.item() for s in map5095_pc]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(classes, map5095_vals)\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"mAP@0.5:0.95\")\n",
    "plt.title(\"Per-Class mAP@0.5:0.95\")\n",
    "plt.xticks(classes)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"map50_95_per_class.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "55342f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 50% of test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/111 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [03:10<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: 0.28357309103012085\n",
      "map_50: 0.4393640160560608\n",
      "map_75: 0.32711005210876465\n",
      "map_small: 0.12810847163200378\n",
      "map_medium: 0.33169397711753845\n",
      "map_large: 0.43209975957870483\n",
      "mar_1: 0.12031914293766022\n",
      "mar_10: 0.3260807991027832\n",
      "mar_100: 0.35826271772384644\n",
      "mar_small: 0.15931354463100433\n",
      "mar_medium: 0.4194425940513611\n",
      "mar_large: 0.44333332777023315\n",
      "map_per_class: -1.0\n",
      "mar_100_per_class: -1.0\n",
      "classes: [0, 1, 2, 3]\n",
      "\n",
      "[Warning] PR curve generation not supported by current torchmetrics version.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Setup ‚Äî‚Äî‚Äî\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUTPUT_DIR = \"evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model.eval()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Evaluation metric (mAP@0.5 and 0.5:0.95 combined) ‚Äî‚Äî‚Äî\n",
    "metric = MeanAveragePrecision()\n",
    "metric.to(device)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Only use 50% of the test data ‚Äî‚Äî‚Äî\n",
    "half_test_data = len(test_loader) // 10\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "print(\"Evaluating on 50% of test data...\")\n",
    "for _ in tqdm(range(half_test_data)):\n",
    "    images, targets = next(test_iter)\n",
    "    images = [img.to(device) for img in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    metric.update(outputs, targets)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Compute metrics ‚Äî‚Äî‚Äî\n",
    "metrics = metric.compute()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Save core metrics ‚Äî‚Äî‚Äî\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_summary.txt\"), \"w\") as f:\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            val = v.item() if v.numel() == 1 else v.cpu().numpy().tolist()\n",
    "        else:\n",
    "            val = v\n",
    "        f.write(f\"{k}: {val}\\n\")\n",
    "        print(f\"{k}: {val}\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Plot PR Curve (Precision vs Recall) ‚Äî‚Äî‚Äî\n",
    "# We'll create this manually using collected values\n",
    "if 'precision_recall_curve' in metric.__dir__():\n",
    "    pr_curves = metric.precision_recall_curve()\n",
    "    precisions = pr_curves[\"precision\"].cpu().numpy()\n",
    "    recalls = pr_curves[\"recall\"].cpu().numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recalls, precisions, label=\"PR Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve (PR)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"pr_curve.png\"))\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"\\n[Warning] PR curve generation not supported by current torchmetrics version.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1512c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating mAP on 50% of test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1111/1111 [30:59<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@0.5     : 0.5057\n",
      "mAP@[0.5:0.95]: 0.3075\n",
      "Collecting predictions for PR curve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1111/1111 [31:08<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All metrics and curves saved in evaluation_graphs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.ops import box_iou\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Setup ‚Äî‚Äî‚Äî\n",
    "OUTPUT_DIR = \"evaluation_graphs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Use 50% of the test dataset ‚Äî‚Äî‚Äî\n",
    "half_len = len(test_loader.dataset) \n",
    "subset_dataset = Subset(test_loader.dataset, list(range(half_len)))\n",
    "subset_loader  = DataLoader(\n",
    "    subset_dataset,\n",
    "    batch_size=test_loader.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=test_loader.collate_fn\n",
    ")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 1) Compute mAP metrics ‚Äî‚Äî‚Äî\n",
    "metric50   = MeanAveragePrecision(class_metrics=False, iou_thresholds=[0.5])\n",
    "metric5095 = MeanAveragePrecision(class_metrics=False)  # default [0.5,...,0.95]\n",
    "\n",
    "print(\"Evaluating mAP on 50% of test data...\")\n",
    "for images, targets in tqdm(subset_loader):\n",
    "    imgs = [img.to(device) for img in images]\n",
    "    tars = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(imgs)\n",
    "    metric50.update(outputs, tars)\n",
    "    metric5095.update(outputs, tars)\n",
    "\n",
    "res50   = metric50.compute()\n",
    "res5095 = metric5095.compute()\n",
    "\n",
    "map50   = res50[\"map\"].item()\n",
    "map5095 = res5095[\"map\"].item()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Save mAP results ‚Äî‚Äî‚Äî\n",
    "with open(os.path.join(OUTPUT_DIR, \"map_results.txt\"), \"w\") as f:\n",
    "    f.write(f\"mAP@0.5     : {map50:.4f}\\n\")\n",
    "    f.write(f\"mAP@[0.5:0.95]: {map5095:.4f}\\n\")\n",
    "print(f\"mAP@0.5     : {map50:.4f}\")\n",
    "print(f\"mAP@[0.5:0.95]: {map5095:.4f}\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Bar chart for mAP comparison ‚Äî‚Äî‚Äî\n",
    "plt.figure()\n",
    "plt.bar([\"mAP@0.5\", \"mAP@[0.5:0.95]\"], [map50, map5095], width=0.4)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"mAP\")\n",
    "plt.title(\"Overall mAP Comparison\")\n",
    "for i, v in enumerate([map50, map5095]):\n",
    "    plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"mAP_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 2) Build detection‚Äêlevel TP/FP for PR curve at IoU=0.5 ‚Äî‚Äî‚Äî\n",
    "all_scores = []\n",
    "all_labels = []  # 1 for TP, 0 for FP\n",
    "\n",
    "print(\"Collecting predictions for PR curve...\")\n",
    "for images, targets in tqdm(subset_loader):\n",
    "    imgs = [img.to(device) for img in images]\n",
    "    tars = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(imgs)\n",
    "\n",
    "    # per-image matching\n",
    "    for out, tar in zip(outputs, tars):\n",
    "        pred_boxes  = out[\"boxes\"].cpu()\n",
    "        pred_scores = out[\"scores\"].cpu()\n",
    "        pred_labels = out[\"labels\"].cpu()\n",
    "        gt_boxes    = tar[\"boxes\"].cpu()\n",
    "        gt_labels   = tar[\"labels\"].cpu()\n",
    "\n",
    "        # sort by score descending\n",
    "        order = torch.argsort(pred_scores, descending=True)\n",
    "        pred_boxes  = pred_boxes[order]\n",
    "        pred_scores = pred_scores[order]\n",
    "        pred_labels = pred_labels[order]\n",
    "\n",
    "        matched_gt = torch.zeros(len(gt_boxes), dtype=torch.bool)\n",
    "\n",
    "        for pb, ps, pl in zip(pred_boxes, pred_scores, pred_labels):\n",
    "            # find GT of same class\n",
    "            mask = gt_labels == pl\n",
    "            if not mask.any():\n",
    "                all_scores.append(ps.item()); all_labels.append(0)\n",
    "                continue\n",
    "\n",
    "            # compute IoUs\n",
    "            ious = box_iou(pb.unsqueeze(0), gt_boxes[mask])\n",
    "            max_iou, idx = ious[0].max(0)\n",
    "            if max_iou >= 0.5:\n",
    "                # get index in original gt_boxes\n",
    "                gt_idx = torch.nonzero(mask)[idx]\n",
    "                if not matched_gt[gt_idx]:\n",
    "                    all_scores.append(ps.item()); all_labels.append(1)\n",
    "                    matched_gt[gt_idx] = True\n",
    "                else:\n",
    "                    # duplicate detection ‚Üí false positive\n",
    "                    all_scores.append(ps.item()); all_labels.append(0)\n",
    "            else:\n",
    "                all_scores.append(ps.item()); all_labels.append(0)\n",
    "\n",
    "# convert to numpy\n",
    "import numpy as np\n",
    "y_scores = np.array(all_scores)\n",
    "y_true   = np.array(all_labels)\n",
    "\n",
    "# compute sklearn PR curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "ap = average_precision_score(y_true, y_scores)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Plot PR curve ‚Äî‚Äî‚Äî\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"AP={ap:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve @ IoU=0.5\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"pr_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Plot Precision vs Threshold ‚Äî‚Äî‚Äî\n",
    "plt.figure()\n",
    "plt.plot(thresholds, precision[:-1])\n",
    "plt.xlabel(\"Confidence Threshold\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Threshold\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"precision_vs_threshold.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Plot Recall vs Threshold ‚Äî‚Äî‚Äî\n",
    "plt.figure()\n",
    "plt.plot(thresholds, recall[:-1])\n",
    "plt.xlabel(\"Confidence Threshold\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall vs Threshold\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"recall_vs_threshold.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úÖ All metrics and curves saved in\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84b02405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##this was final code that working\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# train_box_loss = []\n",
    "# train_class_loss = []\n",
    "# valid_precision = []\n",
    "# valid_recall = []\n",
    "# valid_mAP50 = []\n",
    "# valid_mAP50_95 = []\n",
    "# valid_box_loss = []\n",
    "# valid_class_loss = []\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_train_box_loss = 0\n",
    "#     total_train_class_loss = 0\n",
    "\n",
    "#     for images, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "#         images = [image.to(device) for image in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         outputs = model(images, targets)\n",
    "#         loss_dict = outputs\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         train_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
    "#         train_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
    "\n",
    "#         total_train_box_loss += loss_dict[\"loss_box_reg\"].item()\n",
    "#         total_train_class_loss += loss_dict[\"loss_classifier\"].item()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     total_valid_box_loss = 0\n",
    "#     total_valid_class_loss = 0\n",
    "\n",
    "#     for images, targets in tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "#         images = [image.to(device) for image in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         predicted_boxes = loss_dict[\"boxes\"]\n",
    "#         target_boxes = [t[\"boxes\"] for t in targets]\n",
    "#         giou_loss = GIOU(predicted_boxes, target_boxes)\n",
    "#         losses += giou_loss\n",
    "\n",
    "#         valid_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
    "#         valid_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
    "\n",
    "#         total_valid_box_loss += loss_dict[\"loss_box_reg\"].item()\n",
    "#         total_valid_class_loss += loss_dict[\"loss_classifier\"].item()\n",
    "\n",
    "#         precision = []\n",
    "#         recall = []\n",
    "#         AP50 = []\n",
    "#         AP50_95 = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "\n",
    "#         for i in range(len(targets)):\n",
    "#             target = targets[i]\n",
    "#             output = outputs[i]\n",
    "\n",
    "#             pred_boxes = output[\"boxes\"]\n",
    "#             pred_scores = output[\"scores\"]\n",
    "#             pred_labels = output[\"labels\"]\n",
    "\n",
    "#             true_boxes = target[\"boxes\"]\n",
    "#             true_labels = target[\"labels\"]\n",
    "\n",
    "#             for label in range(7):\n",
    "#                 true_boxes_label = true_boxes[true_labels == label]\n",
    "#                 pred_boxes_label = pred_boxes[pred_labels == label]\n",
    "#                 pred_scores_label = pred_scores[pred_labels == label]\n",
    "\n",
    "#                 if len(true_boxes_label) == 0 or len(pred_boxes_label) == 0:\n",
    "#                     precision.append(0)\n",
    "#                     recall.append(0)\n",
    "#                     AP50.append(0)\n",
    "#                     AP50_95.append(0)\n",
    "#                     continue\n",
    "\n",
    "#                 iou = GIOU(true_boxes_label, pred_boxes_label)\n",
    "#                 iou_thresholds = torch.linspace(0.5, 0.95, 10).to(device)\n",
    "\n",
    "#                 true_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_negatives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "\n",
    "#                 for j, iou_threshold in enumerate(iou_thresholds):\n",
    "#                     true_positives[j] = torch.sum(iou > iou_threshold)\n",
    "#                     false_positives[j] = torch.sum(iou <= iou_threshold)\n",
    "#                     false_negatives[j] = len(true_boxes_label) - true_positives[j]\n",
    "\n",
    "#                 precision.append(true_positives / (true_positives + false_positives + 1e-6))\n",
    "#                 recall.append(true_positives / (true_positives + false_negatives + 1e-6))\n",
    "#                 AP50.append((precision[-1] * recall[-1]).mean())\n",
    "#                 AP50_95.append((precision[-1] * recall[-1]).mean())\n",
    "\n",
    "#         valid_precision.append(precision[-1])\n",
    "#         valid_recall.append(recall[-1])\n",
    "#         valid_mAP50.append(AP50[-1])\n",
    "#         valid_mAP50_95.append(AP50_95[-1])\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/10\")\n",
    "#     print(f\"Train Box Loss: {total_train_box_loss:.4f}, Class Loss: {total_train_class_loss:.4f}\")\n",
    "#     print(f\"Valid Box Loss: {total_valid_box_loss:.4f}, Class Loss: {total_valid_class_loss:.4f}\")\n",
    "#     if valid_mAP50:\n",
    "#         print(f\"Valid mAP@0.5: {valid_mAP50[-1]:.4f}, mAP@0.5:0.95: {valid_mAP50_95[-1]:.4f}\")\n",
    "#     print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9866fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# train_box_loss = []\n",
    "# train_class_loss = []\n",
    "# valid_precision = []\n",
    "# valid_recall = []\n",
    "# valid_mAP50 = []\n",
    "# valid_mAP50_95 = []\n",
    "# valid_box_loss = []\n",
    "# valid_class_loss = []\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_train_box_loss = 0\n",
    "#     total_train_class_loss = 0\n",
    "\n",
    "#     for images, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "#         images = [image.to(device) for image in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         outputs = model(images, targets)\n",
    "#         loss_dict = outputs\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         train_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
    "#         train_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
    "\n",
    "#         total_train_box_loss += loss_dict[\"loss_box_reg\"].item()\n",
    "#         total_train_class_loss += loss_dict[\"loss_classifier\"].item()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     total_valid_box_loss = 0\n",
    "#     total_valid_class_loss = 0\n",
    "\n",
    "#     for images, targets in tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "#         images = [image.to(device) for image in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         predicted_boxes = loss_dict[\"boxes\"]\n",
    "#         target_boxes = [t[\"boxes\"] for t in targets]\n",
    "#         giou_loss = GIOU(predicted_boxes, target_boxes)\n",
    "#         losses += giou_loss\n",
    "\n",
    "#         valid_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
    "#         valid_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
    "\n",
    "#         total_valid_box_loss += loss_dict[\"loss_box_reg\"].item()\n",
    "#         total_valid_class_loss += loss_dict[\"loss_classifier\"].item()\n",
    "\n",
    "#         precision = []\n",
    "#         recall = []\n",
    "#         AP50 = []\n",
    "#         AP50_95 = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "\n",
    "#         for i in range(len(targets)):\n",
    "#             target = targets[i]\n",
    "#             output = outputs[i]\n",
    "\n",
    "#             pred_boxes = output[\"boxes\"]\n",
    "#             pred_scores = output[\"scores\"]\n",
    "#             pred_labels = output[\"labels\"]\n",
    "\n",
    "#             true_boxes = target[\"boxes\"]\n",
    "#             true_labels = target[\"labels\"]\n",
    "\n",
    "#             for label in range(7):\n",
    "#                 true_boxes_label = true_boxes[true_labels == label]\n",
    "#                 pred_boxes_label = pred_boxes[pred_labels == label]\n",
    "#                 pred_scores_label = pred_scores[pred_labels == label]\n",
    "\n",
    "#                 if len(true_boxes_label) == 0 or len(pred_boxes_label) == 0:\n",
    "#                     precision.append(0)\n",
    "#                     recall.append(0)\n",
    "#                     AP50.append(0)\n",
    "#                     AP50_95.append(0)\n",
    "#                     continue\n",
    "\n",
    "#                 iou = GIOU(true_boxes_label, pred_boxes_label)\n",
    "#                 iou_thresholds = torch.linspace(0.5, 0.95, 10).to(device)\n",
    "\n",
    "#                 true_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_negatives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "\n",
    "#                 for j, iou_threshold in enumerate(iou_thresholds):\n",
    "#                     true_positives[j] = torch.sum(iou > iou_threshold)\n",
    "#                     false_positives[j] = torch.sum(iou <= iou_threshold)\n",
    "#                     false_negatives[j] = len(true_boxes_label) - true_positives[j]\n",
    "\n",
    "#                 precision.append(true_positives / (true_positives + false_positives + 1e-6))\n",
    "#                 recall.append(true_positives / (true_positives + false_negatives + 1e-6))\n",
    "#                 AP50.append((precision[-1] * recall[-1]).mean())\n",
    "#                 AP50_95.append((precision[-1] * recall[-1]).mean())\n",
    "\n",
    "#         valid_precision.append(precision[-1])\n",
    "#         valid_recall.append(recall[-1])\n",
    "#         valid_mAP50.append(AP50[-1])\n",
    "#         valid_mAP50_95.append(AP50_95[-1])\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/10\")\n",
    "#     print(f\"Train Box Loss: {total_train_box_loss:.4f}, Class Loss: {total_train_class_loss:.4f}\")\n",
    "#     print(f\"Valid Box Loss: {total_valid_box_loss:.4f}, Class Loss: {total_valid_class_loss:.4f}\")\n",
    "#     if valid_mAP50:\n",
    "#         print(f\"Valid mAP@0.5: {valid_mAP50[-1]:.4f}, mAP@0.5:0.95: {valid_mAP50_95[-1]:.4f}\")\n",
    "#     print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17181e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a196d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f15ef75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this was working\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_box_loss = []\n",
    "# train_class_loss = []\n",
    "# valid_precision = []\n",
    "# valid_recall = []\n",
    "# valid_mAP50 = []\n",
    "# valid_mAP50_95 = []\n",
    "# valid_box_loss = []\n",
    "# valid_class_loss = []\n",
    "\n",
    "# for epoch in range(10):\n",
    "\n",
    "#     model.train()\n",
    "#     # Assume you have a data loader that provides images and targets (bounding boxes and labels)\n",
    "#     for images, targets in train_loader:\n",
    "#     # Ensure that the images and targets are sent to the correct device (CPU or GPU)\n",
    "#         images = [image.to(device) for image in images]  # Assuming images is a list\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Sending targets to device\n",
    "\n",
    "#     # Forward pass through the model\n",
    "#         outputs = model(images, targets)  # targets are now passed\n",
    "\n",
    "#     # Now you can calculate losses or use the predictions from outputs\n",
    "#         loss_dict = outputs\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "#     # Perform the rest of your training steps\n",
    "\n",
    "\n",
    "# # Sum the losses from the model output (if necessary)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "#         train_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
    "#         train_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     for images, targets in valid_loader:\n",
    "#         images = list(image.to(device) for image in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         predicted_boxes = loss_dict[\"boxes\"]\n",
    "#         target_boxes = [t[\"boxes\"] for t in targets]\n",
    "#         giou_loss = GIOU(predicted_boxes, target_boxes)\n",
    "#         losses += giou_loss\n",
    "        \n",
    "#         valid_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
    "#         valid_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
    "        \n",
    "#         images = list(image.to(device) for image in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "#         precision = []\n",
    "#         recall = []\n",
    "#         AP50 = []\n",
    "#         AP50_95 = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "            \n",
    "#         for i in range(len(targets)):\n",
    "#             target = targets[i]\n",
    "#             output = outputs[i]\n",
    "            \n",
    "#             pred_boxes = output[\"boxes\"]\n",
    "#             pred_scores = output[\"scores\"]\n",
    "#             pred_labels = output[\"labels\"]\n",
    "            \n",
    "#             true_boxes = target[\"boxes\"]\n",
    "#             true_labels = target[\"labels\"]\n",
    "            \n",
    "#             for label in range(7):\n",
    "#                 true_boxes_label = true_boxes[true_labels == label]\n",
    "#                 pred_boxes_label = pred_boxes[pred_labels == label]\n",
    "#                 pred_scores_label = pred_scores[pred_labels == label]\n",
    "                \n",
    "#                 if len(true_boxes_label) == 0:\n",
    "#                     precision.append(0)\n",
    "#                     recall.append(0)\n",
    "#                     AP50.append(0)\n",
    "#                     AP50_95.append(0)\n",
    "#                     continue\n",
    "                \n",
    "#                 if len(pred_boxes_label) == 0:\n",
    "#                     precision.append(0)\n",
    "#                     recall.append(0)\n",
    "#                     AP50.append(0)\n",
    "#                     AP50_95.append(0)\n",
    "#                     continue\n",
    "                \n",
    "#                 iou = GIOU(true_boxes_label, pred_boxes_label)\n",
    "#                 iou_thresholds = torch.linspace(0.5, 0.95, 10).to(device)\n",
    "                \n",
    "#                 true_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "#                 false_negatives = torch.zeros(len(iou_thresholds)).to(device)\n",
    "                \n",
    "#                 for i, iou_threshold in enumerate(iou_thresholds):\n",
    "#                     true_positives[i] = torch.sum(iou > iou_threshold)\n",
    "#                     false_positives[i] = torch.sum(iou <= iou_threshold)\n",
    "#                     false_negatives[i] = len(true_boxes_label) - true_positives[i]\n",
    "                    \n",
    "#                 precision.append(true_positives / (true_positives + false_positives))\n",
    "#                 recall.append(true_positives / (true_positives + false_negatives))\n",
    "                \n",
    "#                 AP50.append((precision[-1] * recall[-1]).mean())\n",
    "#                 AP50_95.append((precision[-1] * recall[-1]).mean())\n",
    "                \n",
    "#         valid_precision.append(precision[-1])\n",
    "#         valid_recall.append(recall[-1])\n",
    "#         valid_mAP50.append(AP50[-1])\n",
    "#         valid_mAP50_95.append(AP50_95[-1])\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1}/10\")\n",
    "#     print(f\"Train Box Loss: {total_train_box_loss:.4f}, Class Loss: {total_train_class_loss:.4f}\")\n",
    "#     print(f\"Valid Box Loss: {total_valid_box_loss:.4f}, Class Loss: {total_valid_class_loss:.4f}\")\n",
    "#     if valid_mAP50:\n",
    "#         print(f\"Valid mAP@0.5: {valid_mAP50[-1]:.4f}, mAP@0.5:0.95: {valid_mAP50_95[-1]:.4f}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93bedc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lists to record metrics\n",
    "# train_box_loss = []\n",
    "# train_class_loss = []\n",
    "# valid_precision = []\n",
    "# valid_recall = []\n",
    "# valid_mAP50 = []\n",
    "# valid_mAP50_95 = []\n",
    "# valid_box_loss = []\n",
    "# valid_class_loss = []\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     # Sanity‚Äêcheck printouts\n",
    "#     print(f\"\\n=== Starting Epoch {epoch+1}/10 ===\")\n",
    "#     print(f\"  Train batches: {len(train_loader)}, Valid batches: {len(valid_loader)}\")\n",
    "\n",
    "#     # Reset epoch‚Äêlevel accumulators\n",
    "#     total_train_box_loss = 0.0\n",
    "#     total_train_class_loss = 0.0\n",
    "#     total_valid_box_loss = 0.0\n",
    "#     total_valid_class_loss = 0.0\n",
    "\n",
    "#     # -------------------\n",
    "#     #  TRAINING PHASE\n",
    "#     # -------------------\n",
    "#     model.train()\n",
    "#     for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "#         if batch_idx == 0:\n",
    "#             print(\"  üëâ Entered train loop\")\n",
    "#         # Move data to device\n",
    "#         images = [img.to(device) for img in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         # Forward + loss\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         # GIoU loss addition\n",
    "#         predicted_boxes = loss_dict.get(\"boxes\", None)\n",
    "#         if predicted_boxes is not None:\n",
    "#             giou_loss = GIOU(predicted_boxes, [t[\"boxes\"] for t in targets])\n",
    "#             losses += giou_loss\n",
    "\n",
    "#         # Extract scalar losses safely\n",
    "#         box_loss_val   = loss_dict.get(\"loss_box_reg\", torch.tensor(0.0)).item()\n",
    "#         class_loss_val = loss_dict.get(\"loss_classifier\", torch.tensor(0.0)).item()\n",
    "\n",
    "#         # Record & accumulate\n",
    "#         train_box_loss.append(box_loss_val)\n",
    "#         train_class_loss.append(class_loss_val)\n",
    "#         total_train_box_loss   += box_loss_val\n",
    "#         total_train_class_loss += class_loss_val\n",
    "\n",
    "#         # Backprop\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # -------------------\n",
    "#     #  VALIDATION PHASE\n",
    "#     # -------------------\n",
    "#     model.eval()\n",
    "#     for batch_idx, (images, targets) in enumerate(valid_loader):\n",
    "#         if batch_idx == 0:\n",
    "#             print(\"  üëâ Entered valid loop\")\n",
    "#         images = [img.to(device) for img in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         # Forward + loss\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "#         predicted_boxes = loss_dict.get(\"boxes\", None)\n",
    "#         if predicted_boxes is not None:\n",
    "#             losses += GIOU(predicted_boxes, [t[\"boxes\"] for t in targets])\n",
    "\n",
    "#         # Extract losses\n",
    "#         box_loss_val   = loss_dict.get(\"loss_box_reg\", torch.tensor(0.0)).item()\n",
    "#         class_loss_val = loss_dict.get(\"loss_classifier\", torch.tensor(0.0)).item()\n",
    "\n",
    "#         valid_box_loss.append(box_loss_val)\n",
    "#         valid_class_loss.append(class_loss_val)\n",
    "#         total_valid_box_loss   += box_loss_val\n",
    "#         total_valid_class_loss += class_loss_val\n",
    "\n",
    "#         # Compute detection metrics\n",
    "#         precision_list = []\n",
    "#         recall_list    = []\n",
    "#         AP50_list      = []\n",
    "#         AP50_95_list   = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "\n",
    "#         for i in range(len(targets)):\n",
    "#             tgt    = targets[i]\n",
    "#             pred   = outputs[i]\n",
    "#             tb     = tgt[\"boxes\"]\n",
    "#             tl     = tgt[\"labels\"]\n",
    "#             pb     = pred[\"boxes\"]\n",
    "#             pl     = pred[\"labels\"]\n",
    "\n",
    "#             for lab in range(len(classes)):\n",
    "#                 true_b = tb[tl == lab]\n",
    "#                 pred_b = pb[pl == lab]\n",
    "\n",
    "#                 if len(true_b) == 0 or len(pred_b) == 0:\n",
    "#                     precision_list.append(0)\n",
    "#                     recall_list.append(0)\n",
    "#                     AP50_list.append(0)\n",
    "#                     AP50_95_list.append(0)\n",
    "#                     continue\n",
    "\n",
    "#                 iou          = GIOU(true_b, pred_b)\n",
    "#                 thresholds   = torch.linspace(0.5, 0.95, 10).to(device)\n",
    "#                 tp = torch.zeros(len(thresholds), device=device)\n",
    "#                 fp = torch.zeros(len(thresholds), device=device)\n",
    "#                 fn = torch.zeros(len(thresholds), device=device)\n",
    "\n",
    "#                 for j, th in enumerate(thresholds):\n",
    "#                     tp[j] = (iou > th).sum()\n",
    "#                     fp[j] = (iou <= th).sum()\n",
    "#                     fn[j] = len(true_b) - tp[j]\n",
    "\n",
    "#                 prec = tp / (tp + fp + 1e-6)\n",
    "#                 rec  = tp / (tp + fn + 1e-6)\n",
    "#                 precision_list.append(prec.mean().item())\n",
    "#                 recall_list.append(rec.mean().item())\n",
    "#                 AP50_list.append((prec * rec).mean().item())\n",
    "#                 AP50_95_list.append((prec * rec).mean().item())\n",
    "\n",
    "#         valid_precision.append(precision_list[-1])\n",
    "#         valid_recall.append(recall_list[-1])\n",
    "#         valid_mAP50.append(AP50_list[-1])\n",
    "#         valid_mAP50_95.append(AP50_95_list[-1])\n",
    "\n",
    "#     # -------------------\n",
    "#     #  EPOCH SUMMARY\n",
    "#     # -------------------\n",
    "#     print(f\"Epoch {epoch+1}/10 summary:\")\n",
    "#     print(f\"  Train Box Loss: {total_train_box_loss:.4f}, Class Loss: {total_train_class_loss:.4f}\")\n",
    "#     print(f\"  Valid Box Loss: {total_valid_box_loss:.4f}, Class Loss: {total_valid_class_loss:.4f}\")\n",
    "#     if valid_mAP50:\n",
    "#         print(f\"  Valid mAP@0.5: {valid_mAP50[-1]:.4f}, mAP@0.5-0.95: {valid_mAP50_95[-1]:.4f}\")\n",
    "#     print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1536f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# train_box_loss = []\n",
    "# train_class_loss = []\n",
    "# valid_precision = []\n",
    "# valid_recall = []\n",
    "# valid_mAP50 = []\n",
    "# valid_mAP50_95 = []\n",
    "# valid_box_loss = []\n",
    "# valid_class_loss = []\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Reset accumulators\n",
    "#     total_train_box_loss = 0.0\n",
    "#     total_train_class_loss = 0.0\n",
    "#     total_valid_box_loss = 0.0\n",
    "#     total_valid_class_loss = 0.0\n",
    "\n",
    "#     # ----- TRAINING -----\n",
    "#     model.train()\n",
    "#     train_bar = tqdm(train_loader,\n",
    "#                      desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\",\n",
    "#                      leave=False)\n",
    "#     for batch_idx, (images, targets) in enumerate(train_bar, start=1):\n",
    "#         images = [img.to(device) for img in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         # GIoU\n",
    "#         if \"boxes\" in loss_dict:\n",
    "#             giou = GIOU(loss_dict[\"boxes\"], [t[\"boxes\"] for t in targets])\n",
    "#             losses += giou\n",
    "\n",
    "#         # Safe loss extraction\n",
    "#         box_loss_val   = loss_dict.get(\"loss_box_reg\", torch.tensor(0.0)).item()\n",
    "#         class_loss_val = loss_dict.get(\"loss_classifier\", torch.tensor(0.0)).item()\n",
    "\n",
    "#         train_box_loss.append(box_loss_val)\n",
    "#         train_class_loss.append(class_loss_val)\n",
    "#         total_train_box_loss   += box_loss_val\n",
    "#         total_train_class_loss += class_loss_val\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_bar.set_postfix({\n",
    "#             \"BoxLoss\": f\"{total_train_box_loss/batch_idx:.4f}\",\n",
    "#             \"ClsLoss\": f\"{total_train_class_loss/batch_idx:.4f}\"\n",
    "#         })\n",
    "\n",
    "#     # ----- VALIDATION -----\n",
    "#     model.eval()\n",
    "#     valid_bar = tqdm(valid_loader,\n",
    "#                      desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\",\n",
    "#                      leave=False)\n",
    "#     for batch_idx, (images, targets) in enumerate(valid_bar, start=1):\n",
    "#         images = [img.to(device) for img in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         if \"boxes\" in loss_dict:\n",
    "#             giou = GIOU(loss_dict[\"boxes\"], [t[\"boxes\"] for t in targets])\n",
    "#             losses += giou\n",
    "\n",
    "#         box_loss_val   = loss_dict.get(\"loss_box_reg\", torch.tensor(0.0)).item()\n",
    "#         class_loss_val = loss_dict.get(\"loss_classifier\", torch.tensor(0.0)).item()\n",
    "\n",
    "#         valid_box_loss.append(box_loss_val)\n",
    "#         valid_class_loss.append(class_loss_val)\n",
    "#         total_valid_box_loss   += box_loss_val\n",
    "#         total_valid_class_loss += class_loss_val\n",
    "\n",
    "#         # detection metrics\n",
    "#         precision_list = []\n",
    "#         recall_list    = []\n",
    "#         AP50_list      = []\n",
    "#         AP50_95_list   = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "\n",
    "#         for i in range(len(targets)):\n",
    "#             tgt = targets[i]\n",
    "#             pred = outputs[i]\n",
    "#             tb, tl = tgt[\"boxes\"], tgt[\"labels\"]\n",
    "#             pb, pl = pred[\"boxes\"], pred[\"labels\"]\n",
    "\n",
    "#             for lab in range(len(classes)):\n",
    "#                 true_b = tb[tl == lab]\n",
    "#                 pred_b = pb[pl == lab]\n",
    "\n",
    "#                 if len(true_b) == 0 or len(pred_b) == 0:\n",
    "#                     precision_list.append(0)\n",
    "#                     recall_list.append(0)\n",
    "#                     AP50_list.append(0)\n",
    "#                     AP50_95_list.append(0)\n",
    "#                     continue\n",
    "\n",
    "#                 iou = GIOU(true_b, pred_b)\n",
    "#                 ths = torch.linspace(0.5, 0.95, 10).to(device)\n",
    "\n",
    "#                 tp = (iou > ths[:, None]).sum(dim=1)\n",
    "#                 fp = (iou <= ths[:, None]).sum(dim=1)\n",
    "#                 fn = len(true_b) - tp\n",
    "\n",
    "#                 prec = (tp / (tp + fp + 1e-6)).mean().item()\n",
    "#                 rec  = (tp / (tp + fn + 1e-6)).mean().item()\n",
    "#                 pm50 = (prec * rec)\n",
    "#                 pm9595 = pm50  # simple proxy\n",
    "\n",
    "#                 precision_list.append(prec)\n",
    "#                 recall_list.append(rec)\n",
    "#                 AP50_list.append(pm50)\n",
    "#                 AP50_95_list.append(pm9595)\n",
    "\n",
    "#         valid_precision.append(precision_list[-1])\n",
    "#         valid_recall.append(recall_list[-1])\n",
    "#         valid_mAP50.append(AP50_list[-1])\n",
    "#         valid_mAP50_95.append(AP50_95_list[-1])\n",
    "\n",
    "#         valid_bar.set_postfix({\n",
    "#             \"BoxLoss\": f\"{total_valid_box_loss/batch_idx:.4f}\",\n",
    "#             \"ClsLoss\": f\"{total_valid_class_loss/batch_idx:.4f}\",\n",
    "#             \"mAP50\":  f\"{valid_mAP50[-1]:.3f}\"\n",
    "#         })\n",
    "\n",
    "#     # ----- EPOCH SUMMARY -----\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} summary:\")\n",
    "#     print(f\"  Train ‚ñ∂ Box: {total_train_box_loss:.4f}, Cls: {total_train_class_loss:.4f}\")\n",
    "#     print(f\"  Valid ‚ñ∂ Box: {total_valid_box_loss:.4f}, Cls: {total_valid_class_loss:.4f}\")\n",
    "#     print(f\"  mAP@0.5: {valid_mAP50[-1]:.4f}, mAP@0.5-0.95: {valid_mAP50_95[-1]:.4f}\")\n",
    "#     print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2e735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c3040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b5b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a139944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c4f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b18869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2908457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30e2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf9c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c32f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b2b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
